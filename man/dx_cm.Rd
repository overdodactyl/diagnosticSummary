% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dx_metrics.R
\name{dx_cm}
\alias{dx_cm}
\title{Create a Confusion Matrix from Predictions and Truth}
\usage{
dx_cm(predprob, truth, threshold, poslabel)
}
\arguments{
\item{predprob}{Numeric vector of prediction probabilities.}

\item{truth}{Numeric vector of true binary class outcomes.}

\item{threshold}{Numeric value to determine the cutoff for classifying predictions as positive.}

\item{poslabel}{The label of the positive class in the truth data.}
}
\value{
A dataframe object of class "dx_cm" containing the components of the confusion matrix and additional metrics:
\itemize{
  \item \code{tp}: True Positives
  \item \code{fn}: False Negatives
  \item \code{tn}: True Negatives
  \item \code{fp}: False Positives
  \item \code{dispos}: Number of Actual Positives
  \item \code{disneg}: Number of Actual Negatives
  \item \code{n}: Total Number of Observations
  \item \code{correct}: Number of Correct Predictions
  \item \code{testpos}: Number of Predicted Positives
  \item \code{testneg}: Number of Predicted Negatives
}
}
\description{
This function calculates a confusion matrix from predicted probabilities,
true outcomes, a threshold for classification, and a designated positive label.
It calculates true positives, false negatives, true negatives, false positives,
and several other useful metrics.
}
\details{
The function takes predicted probabilities and a threshold to create binary
predictions which are then compared to the true labels to create a confusion matrix.
It is useful for evaluating the performance of a binary classification model.
}
\examples{
# Example usage:
true_labels <- c(1, 0, 1, 1, 0)
predicted_probs <- c(0.9, 0.3, 0.6, 0.8, 0.1)
cm <- dx_cm(predicted_probs, true_labels, threshold = 0.5, poslabel = 1)
print(cm)
}
