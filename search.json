[{"path":"https://overdodactyl.github.io/diagnosticSummary/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2019 Pat Johnson Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Pat Johnson. Author, maintainer. Rickey Carter. Author.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Johnson P, Carter R (2025). diagnosticSummary: Diagnostic Summaries. R package version 0.0.2.13, https://overdodactyl.github.io/diagnosticSummary/.","code":"@Manual{,   title = {diagnosticSummary: Diagnostic Summaries},   author = {Pat Johnson and Rickey Carter},   year = {2025},   note = {R package version 0.0.2.13},   url = {https://overdodactyl.github.io/diagnosticSummary/}, }"},{"path":"https://overdodactyl.github.io/diagnosticSummary/index.html","id":"diagnosticsummary-","dir":"","previous_headings":"","what":"Diagnostic Summaries","title":"Diagnostic Summaries","text":"diagnosticSummary designed quickly create diagnostic summaries reports binary classification data.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Diagnostic Summaries","text":"can install development version GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"overdodactyl/diagnosticSummary\")"},{"path":"https://overdodactyl.github.io/diagnosticSummary/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Diagnostic Summaries","text":"Threshold= 0.3","code":"library(diagnosticSummary) # Load sample data data(\"dx_heart_failure\") head(dx_heart_failure) #>   AgeGroup    Sex truth   predicted           AgeSex predicted_rf #> 1  (20,50]   Male     0 0.016164112   (20,50] - Male   0.19774011 #> 2  (20,50]   Male     0 0.074193671   (20,50] - Male   0.04624277 #> 3  (20,50] Female     0 0.004677979 (20,50] - Female   0.22448980 #> 4  (20,50] Female     0 0.017567313 (20,50] - Female   0.09326425 #> 5  (20,50] Female     0 0.017517025 (20,50] - Female   0.04878049 #> 6  (20,50]   Male     0 0.051570734   (20,50] - Male   0.10982659 # Create dx object dx_obj <- dx(   data = dx_heart_failure,   true_varname = \"truth\",   pred_varname = \"predicted\",   outcome_label = \"Heart Attack\",   threshold_range = c(.1,.2,.3),   setthreshold = .3,   doboot = TRUE,   bootreps = 1000,   grouping_variables = c(\"AgeGroup\", \"Sex\", \"AgeSex\") ) summary(dx_obj, variable = \"Overall\", show_var = F, show_label = F)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/as.data.frame.dx.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert to a data frame — as.data.frame.dx","title":"Convert to a data frame — as.data.frame.dx","text":"Convert data frame","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/as.data.frame.dx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert to a data frame — as.data.frame.dx","text":"","code":"# S3 method for class 'dx' as.data.frame(   x,   row.names = NULL,   optional = TRUE,   thresh = NA,   variable = NA,   label = NA,   measure = NA,   ... )"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/as.data.frame.dx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert to a data frame — as.data.frame.dx","text":"x object class \"dx\" row.names NULL character vector giving row names data frame. Missing values allowed. optional logical. TRUE, setting row names converting column names (syntactic names: see make.names) optional. thresh threshold return values variable Variable include returned values label Labels include returned values measure Measures include ... additional arguments passed methods","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/breslow_day_test.html","id":null,"dir":"Reference","previous_headings":"","what":"Breslow-Day Test for Homogeneity of Odds Ratios — breslow_day_test","title":"Breslow-Day Test for Homogeneity of Odds Ratios — breslow_day_test","text":"Performs Breslow-Day test homogeneity odds ratios across different strata data. Optionally includes Tarone's correction. test useful investigating whether effect explanatory variable outcome consistent across different strata.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/breslow_day_test.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Breslow-Day Test for Homogeneity of Odds Ratios — breslow_day_test","text":"","code":"breslow_day_test(contingency_table, odds_ratio = NA, correct = FALSE)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/breslow_day_test.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Breslow-Day Test for Homogeneity of Odds Ratios — breslow_day_test","text":"contingency_table 2x2xK contingency table K number strata. odds_ratio Optionally, overall odds ratio use test. provided, Mantel-Haenszel odds ratio calculated. correct Logical indicating whether apply Tarone's correction.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/breslow_day_test.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Breslow-Day Test for Homogeneity of Odds Ratios — breslow_day_test","text":"list Breslow-Day test statistic, p-value, method used, data name.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/breslow_day_test.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Breslow-Day Test for Homogeneity of Odds Ratios — breslow_day_test","text":"Breslow NE, Day NE. Statistical methods cancer research. Volume - analysis case-control studies. IARC Sci Publ. 1980;(32):5-338.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/diagnosticSummary.html","id":null,"dir":"Reference","previous_headings":"","what":"diagnosticSummary package — diagnosticSummary","title":"diagnosticSummary package — diagnosticSummary","text":"Diagnostic Summaries","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx.html","id":null,"dir":"Reference","previous_headings":"","what":"Set options for diagnostic analysis — dx","title":"Set options for diagnostic analysis — dx","text":"Use function return list tuning parameters analyze diagnostic test","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set options for diagnostic analysis — dx","text":"","code":"dx(   data,   classlabels = c(\"Negative\", \"Positive\"),   threshold_range = NA,   outcome_label = NA,   pred_varname,   true_varname,   setthreshold = 0.5,   poslabel = 1,   grouping_variables = NA,   prevalence = NA,   citype = \"exact\",   bootreps = 2000,   bootseed = 20191015,   doboot = FALSE,   direction = \"auto\",   ... )"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set options for diagnostic analysis — dx","text":"data tbl. classlabels Labels predicted variable.  Needs 0, 1 order. threshold_range Optional. numeric vector thresholds loop . outcome_label Label outcome (string) pred_varname Column name containing AI prediction (string) true_varname Column name containing AI reference standard (string) setthreshold numeric value representing threshold used identify AI prediction poslabel Positive class.  Variable coded 0/1 1 event grouping_variables Character vector variable names summarized .  Variables converted factors already one. prevalence Numeric value 0 1, representing target prevalence additional NPV PPV calculations. citype Confidence interval type. bootreps Number bootstrap samples used generate F1 score CI bootseed Seed value used calculating bootsraped CI's doboot Logical. Generate bootstrap estimate F1 confidence interval? direction Direction roc comparison.  See ?pROC::roc ... currently unused","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_accuracy.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Accuracy — dx_accuracy","title":"Calculate Accuracy — dx_accuracy","text":"Calculates proportion correct predictions (True Positives + True Negatives) cases confusion matrix object, providing measure classifier's overall correctness.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_accuracy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Accuracy — dx_accuracy","text":"","code":"dx_accuracy(cm, detail = \"full\", ...)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_accuracy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Accuracy — dx_accuracy","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals. ... Additional arguments pass metric_binomial function, citype type confidence interval method.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_accuracy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Accuracy — dx_accuracy","text":"Depending detail parameter, returns numeric value representing calculated metric data frame/tibble detailed diagnostics including confidence intervals possibly metrics relevant understanding metric.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_accuracy.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Accuracy — dx_accuracy","text":"\\(Accuracy = \\frac{True Positives + True Negatives}{Total Cases}\\) Accuracy one intuitive performance measures simply ratio correctly predicted observation total observations. common starting point evaluating performance classifier. However, suitable unbalanced classes due tendency misleadingly high class interest underrepresented. detailed diagnostics, including confidence intervals, specify detail = \"full\".","code":""},{"path":[]},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_accuracy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Accuracy — dx_accuracy","text":"","code":"cm <- dx_cm(   dx_heart_failure$predicted,   dx_heart_failure$predicted,   threshold = 0.3, poslabel = 1 ) simple_accuracy <- dx_accuracy(cm, detail = \"simple\") detailed_accuracy <- dx_accuracy(cm) print(simple_accuracy) #> [1] 0.532567 print(detailed_accuracy) #> # A tibble: 1 × 8 #>   measure  summary          estimate conf_low conf_high fraction conf_type notes #>   <chr>    <chr>               <dbl>    <dbl>     <dbl> <chr>    <chr>     <chr> #> 1 Accuracy 53.3% (47.0%, 5…    0.533    0.470     0.594 139/261  Binomial… \"\""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_auc.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Area Under the ROC Curve (AUC) — dx_auc","title":"Calculate Area Under the ROC Curve (AUC) — dx_auc","text":"Calculates Area Receiver Operating Characteristic (ROC) Curve prediction probabilities true binary outcomes. AUC measure ability classifier distinguish classes used summary ROC curve.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_auc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Area Under the ROC Curve (AUC) — dx_auc","text":"","code":"dx_auc(truth, predprob, detail = \"full\")"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_auc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Area Under the ROC Curve (AUC) — dx_auc","text":"truth Vector true binary class outcomes (0 1). predprob Vector prediction probabilities corresponding true outcomes. detail Character string specifying level detail output: \"simple\" just AUC value, \"full\" AUC value along confidence intervals.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_auc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Area Under the ROC Curve (AUC) — dx_auc","text":"Depending detail parameter, returns single numeric value AUC data frame AUC confidence intervals.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_auc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Area Under the ROC Curve (AUC) — dx_auc","text":"","code":"# Assuming you have a vector of true class labels and predicted probabilities true_classes <- c(1, 0, 1, 1, 0, 0, 1) predicted_probs <- c(0.9, 0.1, 0.8, 0.75, 0.33, 0.25, 0.67) simple_auc <- dx_auc(true_classes, predicted_probs, detail = \"simple\") #> Warning: ci.auc() of a ROC curve with AUC == 1 is always 1-1 and can be misleading. #> Warning: ci.auc() of a ROC curve with AUC == 1 is always 1-1 and can be misleading. detailed_auc <- dx_auc(true_classes, predicted_probs) #> Warning: ci.auc() of a ROC curve with AUC == 1 is always 1-1 and can be misleading. #> Warning: ci.auc() of a ROC curve with AUC == 1 is always 1-1 and can be misleading. print(simple_auc) #> [1] 1 print(detailed_auc) #> # A tibble: 1 × 8 #>   measure summary           estimate conf_low conf_high fraction conf_type notes #>   <chr>   <chr>                <dbl>    <dbl>     <dbl> <chr>    <chr>     <chr> #> 1 AUC ROC 1.000 (1.000, 1.…        1        1         1 \"\"       DeLong    \"\""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_auc_pr.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Area Under the Precision-Recall Curve (AUC-PR) — dx_auc_pr","title":"Calculate Area Under the Precision-Recall Curve (AUC-PR) — dx_auc_pr","text":"function calculates Area Curve (AUC) Precision-Recall curve using trapezoidal rule. ensures proper alignment precision recall values adding starting point recall=0 first observed precision ending point recall=1 last observed precision.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_auc_pr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Area Under the Precision-Recall Curve (AUC-PR) — dx_auc_pr","text":"","code":"dx_auc_pr(precision, recall, detail = \"full\")"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_auc_pr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Area Under the Precision-Recall Curve (AUC-PR) — dx_auc_pr","text":"precision Numeric vector precision values corresponding different thresholds. recall Numeric vector recall values corresponding different thresholds. detail Character string specifying level detail output: \"simple\" just AUC value, \"full\" AUC value along confidence intervals.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_auc_pr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Area Under the Precision-Recall Curve (AUC-PR) — dx_auc_pr","text":"Depending detail parameter, returns single numeric value AUC data frame AUC confidence intervals.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_auc_pr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Area Under the Precision-Recall Curve (AUC-PR) — dx_auc_pr","text":"function prepares precision recall vectors ensuring ordered increasing recall values. calculates AUC using trapezoidal rule, sum areas trapezoids formed consecutive pair points. first last points added cover entire recall range 0 1.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_auc_pr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Area Under the Precision-Recall Curve (AUC-PR) — dx_auc_pr","text":"","code":"# Assuming pr_data is your dataframe with precision and recall columns dx_obj <- dx(   data = dx_heart_failure,   true_varname = \"truth\",   pred_varname = \"predicted\",   outcome_label = \"Heart Attack\",   setthreshold = .3 ) auc_pr <- dx_auc_pr(dx_obj$thresholds$precision, dx_obj$thresholds$sensitivity) print(auc_pr) #> # A tibble: 1 × 8 #>   measure summary estimate conf_low conf_high fraction conf_type notes #>   <chr>   <chr>      <dbl> <lgl>    <lgl>     <chr>    <chr>     <chr> #> 1 AUC PR  0.87       0.873 NA       NA        \"\"       \"\"        \"\""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_balanced_accuracy.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Balanced Accuracy — dx_balanced_accuracy","title":"Calculate Balanced Accuracy — dx_balanced_accuracy","text":"Calculates Balanced Accuracy, average sensitivity (recall) specificity. metric particularly useful imbalanced datasets accounts positive negative classes equally inherently favor majority class.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_balanced_accuracy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Balanced Accuracy — dx_balanced_accuracy","text":"","code":"dx_balanced_accuracy(cm, detail = \"full\", boot = FALSE, bootreps = 1000)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_balanced_accuracy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Balanced Accuracy — dx_balanced_accuracy","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals. boot Logical specifying confidence intervals generated via bootstrapping.  Note, can slow. bootreps number bootstrap replications calculating confidence intervals.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_balanced_accuracy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Balanced Accuracy — dx_balanced_accuracy","text":"Depending detail parameter, returns numeric value representing calculated metric data frame/tibble detailed diagnostics including confidence intervals possibly metrics relevant understanding metric.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_balanced_accuracy.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Balanced Accuracy — dx_balanced_accuracy","text":"Balanced Accuracy mitigates issue regular accuracy metric favoring models predict majority class imbalanced dataset. taking average sensitivity specificity, gives better measure overall performance especially classes imbalanced costs different errors vary greatly. formula Balanced Accuracy : $$Balanced Accuracy = \\frac{Sensitivity + Specificity}{2}$$","code":""},{"path":[]},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_balanced_accuracy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Balanced Accuracy — dx_balanced_accuracy","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth, threshold = 0.5, poslabel = 1) simple_balanced_accuracy <- dx_balanced_accuracy(cm, detail = \"simple\") detailed_balanced_accuracy <- dx_balanced_accuracy(cm) print(simple_balanced_accuracy) #> [1] 0.8070615 print(detailed_balanced_accuracy) #> # A tibble: 1 × 8 #>   measure           summary estimate conf_low conf_high fraction conf_type notes #>   <chr>             <chr>      <dbl>    <dbl>     <dbl> <chr>    <chr>     <chr> #> 1 Balanced Accuracy 0.81       0.807       NA        NA \"\"       NA        Spec…"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_brier.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Brier Score — dx_brier","title":"Calculate Brier Score — dx_brier","text":"Brier score proper score function measures accuracy probabilistic predictions. applicable tasks predictions must assign probabilities set mutually exclusive discrete outcomes. binary classification, Brier score measure far predicted probabilities actual outcomes.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_brier.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Brier Score — dx_brier","text":"","code":"dx_brier(predprob, truth, detail = \"full\")"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_brier.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Brier Score — dx_brier","text":"predprob Numeric vector predicted probabilities associated positive class. truth Numeric vector true binary outcomes, typically 0 1, length predprob. detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_brier.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Brier Score — dx_brier","text":"Depending detail parameter, returns numeric value representing calculated metric data frame/tibble detailed diagnostics including confidence intervals possibly metrics relevant understanding metric.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_brier.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Brier Score — dx_brier","text":"formula Brier score binary classification : $$BS = \\frac{1}{N} \\sum_{=1}^{N} (f_i - o_i)^2$$ : \\(N\\) number predictions, \\(f_i\\) predicted probability occurrence positive class ith prediction, \\(o_i\\) actual outcome ith prediction, 0 1. Brier score ranges 0 1, 0 represents perfect model 1 represents worst model. equivalent mean squared error used regression can decomposed calibration loss, refinement loss, uncertainty. makes informative metric probabilistic forecasts, providing nuanced view model's predictive performance.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_brier.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Brier Score — dx_brier","text":"","code":"predprob <- dx_heart_failure$predicted truth <- dx_heart_failure$truth simple <- dx_brier(predprob, truth, detail = \"simple\") detailed <- dx_brier(predprob, truth) print(simple) #> [1] 0.1137428 print(detailed) #> # A tibble: 1 × 8 #>   measure     summary estimate conf_low conf_high fraction conf_type notes       #>   <chr>       <chr>      <dbl> <lgl>    <lgl>     <chr>    <chr>     <chr>       #> 1 Brier Score 0.11       0.114 NA       NA        \"\"       \"\"        CIs not ye…"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_chi_square.html","id":null,"dir":"Reference","previous_headings":"","what":"Chi-Square Test for Independence in a 2x2 Table — dx_chi_square","title":"Chi-Square Test for Independence in a 2x2 Table — dx_chi_square","text":"Conducts Chi-square test independence 2x2 confusion matrix derived binary classification results, assessing whether observed frequency distribution differs expected distribution.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_chi_square.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chi-Square Test for Independence in a 2x2 Table — dx_chi_square","text":"","code":"dx_chi_square(cm, detail = \"full\")"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_chi_square.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chi-Square Test for Independence in a 2x2 Table — dx_chi_square","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_chi_square.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chi-Square Test for Independence in a 2x2 Table — dx_chi_square","text":"Depending detail parameter: - \"simple\": single numeric value representing p-value Chi-square test. - \"full\": data frame Chi-square test result, including p-value method note.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_chi_square.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Chi-Square Test for Independence in a 2x2 Table — dx_chi_square","text":"Chi-square test used determine whether significant association predicted actual binary classifications. compares observed frequencies cell table frequencies expected rows columns independent. low p-value indicates distributions actual predicted classifications independent, suggesting significant association . function uses Pearson's Chi-squared test Yates' continuity correction default, accurate small sample sizes. test appropriate cell 2x2 table expected frequency 5 .","code":""},{"path":[]},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_chi_square.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chi-Square Test for Independence in a 2x2 Table — dx_chi_square","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth,   threshold = 0.3, poslabel = 1 ) simple <- dx_chi_square(cm, detail = \"simple\") detailed <- dx_chi_square(cm) print(simple) #> [1] 5.450633e-21 print(detailed) #> # A tibble: 1 × 8 #>   measure           summary estimate conf_low conf_high fraction conf_type notes #>   <chr>             <chr>      <dbl> <lgl>    <lgl>     <chr>    <chr>     <chr> #> 1 Pearson's Chi-sq… p<0.01  5.45e-21 NA       NA        \"\"       \"\"        Pear…"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_cm.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Confusion Matrix from Predictions and Truth — dx_cm","title":"Create a Confusion Matrix from Predictions and Truth — dx_cm","text":"function calculates confusion matrix predicted probabilities, true outcomes, threshold classification, designated positive label. calculates true positives, false negatives, true negatives, false positives, several useful metrics.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_cm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Confusion Matrix from Predictions and Truth — dx_cm","text":"","code":"dx_cm(predprob, truth, threshold, poslabel)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_cm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Confusion Matrix from Predictions and Truth — dx_cm","text":"predprob Numeric vector prediction probabilities. truth Numeric vector true binary class outcomes. threshold Numeric value determine cutoff classifying predictions positive. poslabel label positive class truth data.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_cm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Confusion Matrix from Predictions and Truth — dx_cm","text":"dataframe object class \"dx_cm\" containing components confusion matrix additional metrics: tp: True Positives fn: False Negatives tn: True Negatives fp: False Positives dispos: Number Actual Positives disneg: Number Actual Negatives n: Total Number Observations correct: Number Correct Predictions testpos: Number Predicted Positives testneg: Number Predicted Negatives","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_cm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a Confusion Matrix from Predictions and Truth — dx_cm","text":"function takes predicted probabilities threshold create binary predictions compared true labels create confusion matrix. useful evaluating performance binary classification model.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_cm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Confusion Matrix from Predictions and Truth — dx_cm","text":"","code":"# Example usage: true_labels <- c(1, 0, 1, 1, 0) predicted_probs <- c(0.9, 0.3, 0.6, 0.8, 0.1) cm <- dx_cm(predicted_probs, true_labels, threshold = 0.5, poslabel = 1) print(cm) #>   tp fn tn fp dispos disneg n correct testpos testneg #> 1  3  0  2  0      3      2 5       5       3       2"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_cohens_kappa.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Cohen's Kappa — dx_cohens_kappa","title":"Calculate Cohen's Kappa — dx_cohens_kappa","text":"Calculates Cohen's Kappa, statistical measure inter-rater reliability agreement qualitative (categorical) items. generally thought robust measure simple percent agreement calculation since Kappa takes account agreement occurring chance.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_cohens_kappa.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Cohen's Kappa — dx_cohens_kappa","text":"","code":"dx_cohens_kappa(cm, detail = \"full\")"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_cohens_kappa.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Cohen's Kappa — dx_cohens_kappa","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_cohens_kappa.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Cohen's Kappa — dx_cohens_kappa","text":"detail \"simple\", returns single numeric value Cohen's Kappa. detail \"full\", returns list data frame includes Cohen's Kappa, standard error, 95% confidence intervals, interpretative notes.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_cohens_kappa.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Cohen's Kappa — dx_cohens_kappa","text":"Cohen's Kappa used measure agreement two raters classify items mutually exclusive categories. formula Cohen's Kappa : $$kappa = (po - pe) / (1 - pe)$$ \\(po\\) relative observed agreement among raters, \\(pe\\) hypothetical probability chance agreement. value kappa can range -1 (total disagreement) 1 (perfect agreement), 0 indicating amount agreement can expected random chance. Interpretation Cohen's Kappa varies, generally, higher value indicates better agreement. Typical benchmarks interpreting Cohen's Kappa : <0: Less chance agreement 0.-0.2: Slight agreement 0.2-0.4: Fair agreement 0.4-0.6: Moderate agreement 0.6-0.8: Substantial agreement 0.8-1.0: Almost perfect agreement","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_cohens_kappa.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Cohen's Kappa — dx_cohens_kappa","text":"","code":"# Assuming you have a confusion matrix cm with appropriate structure cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth,   threshold =     0.5, poslabel = 1 ) kappa_simple <- dx_cohens_kappa(cm, detail = \"simple\") kappa_full <- dx_cohens_kappa(cm) print(kappa_simple) #> [1] 0.6361249 print(kappa_full) #> # A tibble: 1 × 8 #>   measure       summary     estimate conf_low conf_high fraction conf_type notes #>   <chr>         <chr>          <dbl>    <dbl>     <dbl> <chr>    <chr>     <chr> #> 1 Cohen's Kappa 0.64 (0.54…    0.636    0.537     0.736 \"\"       Standard… Subs…"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_compare.html","id":null,"dir":"Reference","previous_headings":"","what":"Compare Multiple Classification Models — dx_compare","title":"Compare Multiple Classification Models — dx_compare","text":"Compares multiple classification models pairwise using various statistical tests assess differences performance metrics. supports paired unpaired comparisons.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_compare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compare Multiple Classification Models — dx_compare","text":"","code":"dx_compare(dx_list, paired = TRUE)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_compare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compare Multiple Classification Models — dx_compare","text":"dx_list list dx objects representing models compared. dx object result call dx(). paired Logical, indicating whether comparisons treated paired. Paired comparisons appropriate models evaluated set instances (e.g., cross-validation repeated measures).","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_compare.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compare Multiple Classification Models — dx_compare","text":"dx_compare object containing list dx objects data frame pairwise comparison results test conducted.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_compare.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compare Multiple Classification Models — dx_compare","text":"function utility perform comprehensive comparison multiple classification models. Based value paired, perform appropriate tests.  resulting object can used functions like dx_plot_rocs.","code":""},{"path":[]},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_compare.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compare Multiple Classification Models — dx_compare","text":"","code":"dx_glm <- dx(data = dx_heart_failure, true_varname = \"truth\", pred_varname = \"predicted\") dx_rf <- dx(data = dx_heart_failure, true_varname = \"truth\", pred_varname = \"predicted_rf\") dx_list <- list(dx_glm, dx_rf) dx_comp <- dx_compare(dx_list, paired = TRUE) print(dx_comp$tests) #> # A tibble: 2 × 9 #>   models       test  summary p_value estimate conf_low conf_high statistic notes #>   <chr>        <chr> <chr>     <dbl> <chr>       <dbl>     <dbl>     <dbl> <chr> #> 1 Model 1 vs.… DeLo… 0.04 (… 5.89e-4 \"0.0413…   0.0178    0.0649      3.44 \"\"    #> 2 Model 1 vs.… McNe… p=0.02  1.80e-2 \"\"        NA        NA           5.6  \"\""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_delong.html","id":null,"dir":"Reference","previous_headings":"","what":"DeLong's Test for Comparing Two ROC Curves — dx_delong","title":"DeLong's Test for Comparing Two ROC Curves — dx_delong","text":"function applies DeLong's test compare areas two correlated ROC curves, providing statistical approach assess significant difference . particularly useful comparing performance two diagnostic models.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_delong.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"DeLong's Test for Comparing Two ROC Curves — dx_delong","text":"","code":"dx_delong(dx1, dx2, detail = \"full\", paired = TRUE)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_delong.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"DeLong's Test for Comparing Two ROC Curves — dx_delong","text":"dx1 dx object containing first ROC curve. dx2 dx object containing second ROC curve. detail Character specifying level detail output: \"simple\" p-value , \"full\" detailed test results. paired Logical indicating data dx1 dx2 paired.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_delong.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"DeLong's Test for Comparing Two ROC Curves — dx_delong","text":"Depending detail parameter, returns p-value comprehensive summary test including test statistic, p-value, confidence interval difference AUC.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_delong.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"DeLong's Test for Comparing Two ROC Curves — dx_delong","text":"function utilizes roc.test function pROC package perform DeLong's test, suitable comparing two correlated ROC curves. correlation typically arises using set samples generate curves. significant p-value indicates statistically significant difference AUCs two models. detail \"full\", additional information comparison, including estimated difference AUC confidence interval, provided. confidence interval gives range plausible values difference AUC two models.","code":""},{"path":[]},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_delong.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"DeLong's Test for Comparing Two ROC Curves — dx_delong","text":"","code":"dx_glm <- dx(data = dx_heart_failure, true_varname = \"truth\", pred_varname = \"predicted\") dx_rf <- dx(data = dx_heart_failure, true_varname = \"truth\", pred_varname = \"predicted_rf\") simple <- dx_delong(dx_glm, dx_rf, detail = \"simple\") detailed <- dx_delong(dx_glm, dx_rf) print(simple) #> [1] 0.0005886491 print(detailed) #> # A tibble: 1 × 9 #>   models test        summary p_value estimate conf_low conf_high statistic notes #>   <chr>  <chr>       <chr>     <dbl>    <dbl>    <dbl>     <dbl>     <dbl> <chr> #> 1 \"\"     DeLong's t… 0.04 (… 5.89e-4   0.0413   0.0178    0.0649      3.44 \"\""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_detection_prevalence.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Detection Prevalence — dx_detection_prevalence","title":"Calculate Detection Prevalence — dx_detection_prevalence","text":"Calculates Detection Prevalence, proportion cases predicted positive classifier total number cases. Detection Prevalence provides measure often condition identified model, regardless actual prevalence.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_detection_prevalence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Detection Prevalence — dx_detection_prevalence","text":"","code":"dx_detection_prevalence(cm, detail = \"full\", ...)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_detection_prevalence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Detection Prevalence — dx_detection_prevalence","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals. ... Additional arguments pass metric_binomial function, citype type confidence interval method.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_detection_prevalence.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Detection Prevalence — dx_detection_prevalence","text":"Depending detail parameter, returns numeric value representing calculated metric data frame/tibble detailed diagnostics including confidence intervals possibly metrics relevant understanding metric.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_detection_prevalence.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Detection Prevalence — dx_detection_prevalence","text":"Detection Prevalence measure frequency classifier predicts condition positive. different actual condition prevalence population influenced classifier's threshold performance characteristics. High detection prevalence indicate tendency model predict positive cases, might useful detrimental depending specific application cost false positives. important compare Detection Prevalence actual condition prevalence assess model's performance. formula Detection Prevalence : $$Detection Prevalence = \\frac{Number Predicted Positives}{Total Number Cases}$$","code":""},{"path":[]},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_detection_prevalence.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Detection Prevalence — dx_detection_prevalence","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth,   threshold =     0.5, poslabel = 1 ) simple_detection_prevalence <- dx_detection_prevalence(cm, detail = \"simple\") detailed_detection_prevalence <- dx_detection_prevalence(cm) print(simple_detection_prevalence) #> [1] 0.3103448 print(detailed_detection_prevalence) #> # A tibble: 1 × 8 #>   measure           summary estimate conf_low conf_high fraction conf_type notes #>   <chr>             <chr>      <dbl>    <dbl>     <dbl> <chr>    <chr>     <chr> #> 1 Detection Preval… 31.0% …    0.310    0.255     0.370 81/261   Binomial… \"\""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_edit_cell.html","id":null,"dir":"Reference","previous_headings":"","what":"Edit a cell within an object returned from dx_forest — dx_edit_cell","title":"Edit a cell within an object returned from dx_forest — dx_edit_cell","text":"convenient wrapper used edit cells gtable.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_edit_cell.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Edit a cell within an object returned from dx_forest — dx_edit_cell","text":"","code":"dx_edit_cell(table, row, col, name = \"core-fg\", ...)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_edit_cell.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Edit a cell within an object returned from dx_forest — dx_edit_cell","text":"table table returned dx_forest row Numeric vector rows edit col Numeric vector columns edit name Name table layer edit ... Parameters passed grid::editGrob grid::gpar hjust.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_f1.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate F1 Score with Confidence Intervals — dx_f1","title":"Calculate F1 Score with Confidence Intervals — dx_f1","text":"Calculates F1 score confusion matrix object option include bootstrapped confidence intervals. F1 score harmonic mean precision recall.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_f1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate F1 Score with Confidence Intervals — dx_f1","text":"","code":"dx_f1(cm, detail = \"full\", boot = FALSE, bootreps = 1000)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_f1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate F1 Score with Confidence Intervals — dx_f1","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals. boot Logical specifying confidence intervals generated via bootstrapping.  Note, can slow. bootreps number bootstrap replications calculating confidence intervals.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_f1.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate F1 Score with Confidence Intervals — dx_f1","text":"Depending detail parameter, returns single numeric value F1 data frame F1 confidence intervals.","code":""},{"path":[]},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_f1.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate F1 Score with Confidence Intervals — dx_f1","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth,   threshold =     0.5, poslabel = 1 ) simple_f <- dx_f1(cm, detail = \"simple\") detailed_f <- dx_f1(cm) print(simple_f) #> [1] 0.7597765 print(detailed_f) #> # A tibble: 1 × 8 #>   measure  summary estimate conf_low conf_high fraction conf_type notes          #>   <chr>    <chr>      <dbl>    <dbl>     <dbl> <chr>    <chr>     <chr>          #> 1 F1 Score 0.76       0.760       NA        NA \"\"       NA        Specify `boot…"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_f2.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate F2 Score with Confidence Intervals — dx_f2","title":"Calculate F2 Score with Confidence Intervals — dx_f2","text":"Calculates F2 score confusion matrix object option include bootstrapped confidence intervals. F2 score weights recall higher precision.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_f2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate F2 Score with Confidence Intervals — dx_f2","text":"","code":"dx_f2(cm, detail = \"full\", boot = FALSE, bootreps = 1000)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_f2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate F2 Score with Confidence Intervals — dx_f2","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals. boot Logical specifying confidence intervals generated via bootstrapping.  Note, can slow. bootreps number bootstrap replications calculating confidence intervals.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_f2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate F2 Score with Confidence Intervals — dx_f2","text":"Depending detail parameter, returns single numeric value F2 data frame F2 confidence intervals.","code":""},{"path":[]},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_f2.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate F2 Score with Confidence Intervals — dx_f2","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth,   threshold =     0.5, poslabel = 1 ) simple_f <- dx_f2(cm, detail = \"simple\") detailed_f <- dx_f2(cm) print(simple_f) #> [1] 0.7188161 print(detailed_f) #> # A tibble: 1 × 8 #>   measure  summary estimate conf_low conf_high fraction conf_type notes          #>   <chr>    <chr>      <dbl>    <dbl>     <dbl> <chr>    <chr>     <chr>          #> 1 F2 Score 0.72       0.719       NA        NA \"\"       NA        Specify `boot…"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_fbeta.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate F-beta Score with Confidence Intervals — dx_fbeta","title":"Calculate F-beta Score with Confidence Intervals — dx_fbeta","text":"Calculates F-beta score confusion matrix object option include bootstrapped confidence intervals. F-beta score generalization F1 score, allowing different importance precision recall via beta parameter.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_fbeta.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate F-beta Score with Confidence Intervals — dx_fbeta","text":"","code":"dx_fbeta(cm, beta = 1, detail = \"full\", boot = FALSE, bootreps = 1000)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_fbeta.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate F-beta Score with Confidence Intervals — dx_fbeta","text":"cm dx_cm object created dx_cm(). beta beta value determining weight precision F-score. detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals. boot Logical specifying confidence intervals generated via bootstrapping.  Note, can slow. bootreps number bootstrap replications calculating confidence intervals.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_fbeta.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate F-beta Score with Confidence Intervals — dx_fbeta","text":"Depending detail parameter, returns single numeric value F-beta data frame F-beta confidence intervals.","code":""},{"path":[]},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_fbeta.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate F-beta Score with Confidence Intervals — dx_fbeta","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth,   threshold =     0.5, poslabel = 1 ) simple_f <- dx_fbeta(cm, beta = .5, detail = \"simple\") detailed_f <- dx_fbeta(cm, beta = .5) print(simple_f) #> [1] 0.8056872 print(detailed_f) #> # A tibble: 1 × 8 #>   measure    summary estimate conf_low conf_high fraction conf_type notes        #>   <chr>      <chr>      <dbl>    <dbl>     <dbl> <chr>    <chr>     <chr>        #> 1 F0.5 Score 0.81       0.806       NA        NA \"\"       NA        Specify `bo…"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_fdr.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate False Discovery Rate (FDR) — dx_fdr","title":"Calculate False Discovery Rate (FDR) — dx_fdr","text":"Calculates False Discovery Rate (FDR), proportion false positives among positive predictions. FDR critical measure many classification contexts, particularly cost false positive high.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_fdr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate False Discovery Rate (FDR) — dx_fdr","text":"","code":"dx_fdr(cm, detail = \"full\", ...)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_fdr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate False Discovery Rate (FDR) — dx_fdr","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals. ... Additional arguments pass metric_binomial function, citype type confidence interval method.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_fdr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate False Discovery Rate (FDR) — dx_fdr","text":"Depending detail parameter, returns numeric value representing calculated metric data frame/tibble detailed diagnostics including confidence intervals possibly metrics relevant understanding metric.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_fdr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate False Discovery Rate (FDR) — dx_fdr","text":"FDR important measure consequences false discoveries (false positives) significant. helps understanding error rate among positive predictions made classifier. lower FDR indicates better precision classifier identifying true positives. formula FDR : $$FDR = \\frac{False Positives}{False Positives + True Positives}$$","code":""},{"path":[]},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_fdr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate False Discovery Rate (FDR) — dx_fdr","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth,   threshold =     0.5, poslabel = 1 ) simple_fdr <- dx_fdr(cm, detail = \"simple\") detailed_fdr <- dx_fdr(cm) print(simple_fdr) #> [1] 0.1604938 print(detailed_fdr) #> # A tibble: 1 × 8 #>   measure           summary estimate conf_low conf_high fraction conf_type notes #>   <chr>             <chr>      <dbl>    <dbl>     <dbl> <chr>    <chr>     <chr> #> 1 False Discovery … 16.0% …    0.160   0.0883     0.259 13/81    Binomial… \"\""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_fishers_exact.html","id":null,"dir":"Reference","previous_headings":"","what":"Fisher's Exact Test for Independence in a 2x2 Table — dx_fishers_exact","title":"Fisher's Exact Test for Independence in a 2x2 Table — dx_fishers_exact","text":"Conducts Fisher's Exact test independence 2x2 confusion matrix derived binary classification results, assessing significance association observed expected frequencies.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_fishers_exact.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fisher's Exact Test for Independence in a 2x2 Table — dx_fishers_exact","text":"","code":"dx_fishers_exact(cm, detail = \"full\")"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_fishers_exact.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fisher's Exact Test for Independence in a 2x2 Table — dx_fishers_exact","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_fishers_exact.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fisher's Exact Test for Independence in a 2x2 Table — dx_fishers_exact","text":"Depending detail parameter: - \"simple\": single numeric value representing p-value Fisher's Exact test. - \"full\": data frame Fisher's Exact test result, including p-value method note.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_fishers_exact.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fisher's Exact Test for Independence in a 2x2 Table — dx_fishers_exact","text":"Fisher's Exact Test used examine significance association variables 2x2 contingency table, particularly useful sample sizes small. Unlike chi-square test, rely large sample distribution approximations hence exact. especially preferred data small expected frequencies one cells table. low p-value indicates significant association predicted actual binary classifications.","code":""},{"path":[]},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_fishers_exact.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fisher's Exact Test for Independence in a 2x2 Table — dx_fishers_exact","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth,   threshold = 0.3, poslabel = 1 ) simple <- dx_fishers_exact(cm, detail = \"simple\") detailed <- dx_fishers_exact(cm) print(simple) #> [1] 1.660279e-22 print(detailed) #> # A tibble: 1 × 8 #>   measure        summary estimate conf_low conf_high fraction conf_type notes    #>   <chr>          <chr>      <dbl> <lgl>    <lgl>     <chr>    <chr>     <chr>    #> 1 Fisher's Exact p<0.01  1.66e-22 NA       NA        \"\"       \"\"        Exact t…"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_fowlkes_mallows.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Fowlkes-Mallows Index — dx_fowlkes_mallows","title":"Calculate Fowlkes-Mallows Index — dx_fowlkes_mallows","text":"Calculates Fowlkes-Mallows Index (FM Index) provided confusion matrix. FM Index geometric mean precision recall, providing balance measure two metrics.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_fowlkes_mallows.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Fowlkes-Mallows Index — dx_fowlkes_mallows","text":"","code":"dx_fowlkes_mallows(cm, detail = \"full\", boot = FALSE, bootreps = 1000)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_fowlkes_mallows.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Fowlkes-Mallows Index — dx_fowlkes_mallows","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals. boot Logical specifying confidence intervals generated via bootstrapping.  Note, can slow. bootreps number bootstrap replications calculating confidence intervals.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_fowlkes_mallows.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Fowlkes-Mallows Index — dx_fowlkes_mallows","text":"Depending detail parameter, returns numeric value representing calculated metric data frame/tibble detailed diagnostics including confidence intervals possibly metrics relevant understanding metric.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_fowlkes_mallows.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Fowlkes-Mallows Index — dx_fowlkes_mallows","text":"Fowlkes-Mallows Index defined geometric mean precision (Positive Predictive Value) recall (True Positive Rate Sensitivity). useful measure want balance precision recall without harshness harmonic mean used F1 score. higher Fowlkes-Mallows Index indicates better precision recall balance. formula Fowlkes-Mallows Index : $$FM = \\sqrt{Precision \\times Recall}$$","code":""},{"path":[]},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_fowlkes_mallows.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Fowlkes-Mallows Index — dx_fowlkes_mallows","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth, threshold = 0.5, poslabel = 1) simple_fm_index <- dx_fowlkes_mallows(cm, detail = \"simple\") detailed_fm_index <- dx_fowlkes_mallows(cm) print(simple_fm_index) #> [1] 0.7632264 print(detailed_fm_index) #> # A tibble: 1 × 8 #>   measure           summary estimate conf_low conf_high fraction conf_type notes #>   <chr>             <chr>      <dbl>    <dbl>     <dbl> <chr>    <chr>     <chr> #> 1 Fowlkes-Mallows … 0.76       0.763       NA        NA \"\"       NA        Spec…"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_g_mean.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate G-mean — dx_g_mean","title":"Calculate G-mean — dx_g_mean","text":"Calculates Geometric Mean (G-mean) provided confusion matrix. G-mean measure model's performance considers sensitivity (True Positive Rate) specificity (True Negative Rate), especially useful imbalanced datasets.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_g_mean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate G-mean — dx_g_mean","text":"","code":"dx_g_mean(cm, detail = \"full\", boot = FALSE, bootreps = 1000)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_g_mean.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate G-mean — dx_g_mean","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals. boot Logical specifying confidence intervals generated via bootstrapping.  Note, can slow. bootreps number bootstrap replications calculating confidence intervals.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_g_mean.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate G-mean — dx_g_mean","text":"Depending detail parameter, returns numeric value representing calculated metric data frame/tibble detailed diagnostics including confidence intervals possibly metrics relevant understanding metric.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_g_mean.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate G-mean — dx_g_mean","text":"G-mean geometric mean sensitivity specificity. tries maximize accuracy two classes keeping accuracies balanced. classifier achieve high G-mean score, must perform well positive negative classes. formula G-mean : $$G-mean = \\sqrt{Sensitivity \\times Specificity}$$","code":""},{"path":[]},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_g_mean.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate G-mean — dx_g_mean","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth, threshold = 0.5, poslabel = 1) simple_g_mean <- dx_g_mean(cm, detail = \"simple\") detailed_g_mean <- dx_g_mean(cm) print(simple_g_mean) #> [1] 0.7990855 print(detailed_g_mean) #> # A tibble: 1 × 8 #>   measure summary estimate conf_low conf_high fraction conf_type notes           #>   <chr>   <chr>      <dbl>    <dbl>     <dbl> <chr>    <chr>     <chr>           #> 1 G-mean  0.8        0.799       NA        NA \"\"       NA        Specify `boot …"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_g_test.html","id":null,"dir":"Reference","previous_headings":"","what":"G-Test (Log-Likelihood Ratio Test) for Independence in 2x2 Table — dx_g_test","title":"G-Test (Log-Likelihood Ratio Test) for Independence in 2x2 Table — dx_g_test","text":"Conducts G-test independence assess goodness fit association two categorical variables 2x2 contingency table. alternative chi-squared test particularly useful dealing small expected frequencies.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_g_test.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"G-Test (Log-Likelihood Ratio Test) for Independence in 2x2 Table — dx_g_test","text":"","code":"dx_g_test(cm, detail = \"full\")"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_g_test.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"G-Test (Log-Likelihood Ratio Test) for Independence in 2x2 Table — dx_g_test","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_g_test.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"G-Test (Log-Likelihood Ratio Test) for Independence in 2x2 Table — dx_g_test","text":"test compares observed frequencies expected frequencies based marginal totals calculates G statistic, follows chi-squared distribution. test especially useful data contains small expected frequencies, might make chi-squared test less accurate. low p-value indicates significant association variables significant difference expected distribution. Caution needed zero counts small samples.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_g_test.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"G-Test (Log-Likelihood Ratio Test) for Independence in 2x2 Table — dx_g_test","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth,   threshold = 0.3, poslabel = 1 ) simple <- dx_g_test(cm, detail = \"simple\") detailed <- dx_g_test(cm) print(simple) #> [1] 5.50871e-23 print(detailed) #> # A tibble: 1 × 8 #>   measure summary estimate conf_low conf_high fraction conf_type notes #>   <chr>   <chr>      <dbl> <lgl>    <lgl>     <chr>    <chr>     <chr> #> 1 G-Test  p<0.01  5.51e-23 NA       NA        \"\"       \"\"        \"\""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_heart_failure.html","id":null,"dir":"Reference","previous_headings":"","what":"Heart attack outcomes and predictions — dx_heart_failure","title":"Heart attack outcomes and predictions — dx_heart_failure","text":"data set containing heart attack outcomes 261 patients along predicted probabilities heart attack.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_heart_failure.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Heart attack outcomes and predictions — dx_heart_failure","text":"","code":"dx_heart_failure"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_heart_failure.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Heart attack outcomes and predictions — dx_heart_failure","text":"data frame 261 rows 5 variables: AgeGroup Age group Sex sex truth Heart failure (outcome) predicted Predicted outcome GLM model predicted_rf Predicted outcome Random Forest model AgeSex Age sex group","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_heart_failure.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Heart attack outcomes and predictions — dx_heart_failure","text":"https://www.kaggle.com/imnikhilanand/heart-attack-prediction","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_lrt_neg.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Negative Likelihood Ratio — dx_lrt_neg","title":"Calculate Negative Likelihood Ratio — dx_lrt_neg","text":"Calculates Negative Likelihood Ratio (LR-) confusion matrix object. LR- compares probability negative test result among patients disease probability negative test result among patients without disease.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_lrt_neg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Negative Likelihood Ratio — dx_lrt_neg","text":"","code":"dx_lrt_neg(cm, detail = \"full\", ...)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_lrt_neg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Negative Likelihood Ratio — dx_lrt_neg","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals. ... Additional arguments pass metric_binomial function, citype type confidence interval method.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_lrt_neg.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Negative Likelihood Ratio — dx_lrt_neg","text":"Depending detail parameter, returns numeric value representing calculated metric data frame/tibble detailed diagnostics including confidence intervals possibly metrics relevant understanding metric.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_lrt_neg.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Negative Likelihood Ratio — dx_lrt_neg","text":"negative likelihood ratio calculated (FN / (TP + FN)) / (TN / (FP + TN)). used assess diagnostic usefulness test. LR- closer 0 indicates good diagnostic test can confidently rule disease test negative.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_lrt_neg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Negative Likelihood Ratio — dx_lrt_neg","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth,   threshold =     0.5, poslabel = 1 ) simple_lrn <- dx_lrt_neg(cm, detail = \"simple\") detailed_lrn <- dx_lrt_neg(cm) print(simple_lrn) #> [1] 0.3326531 print(detailed_lrn) #> # A tibble: 1 × 8 #>   measure summary           estimate conf_low conf_high fraction conf_type notes #>   <chr>   <chr>                <dbl>    <dbl>     <dbl> <chr>    <chr>     <chr> #> 1 LRT-    0.33 (0.25, 0.45)    0.333    0.246     0.450 \"\"       Large sa… \"\""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_lrt_pos.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Positive Likelihood Ratio — dx_lrt_pos","title":"Calculate Positive Likelihood Ratio — dx_lrt_pos","text":"Calculates Positive Likelihood Ratio (LR+) confusion matrix object. LR+ compares probability positive test result among patients disease probability positive test result among patients without disease.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_lrt_pos.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Positive Likelihood Ratio — dx_lrt_pos","text":"","code":"dx_lrt_pos(cm, detail = \"full\", ...)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_lrt_pos.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Positive Likelihood Ratio — dx_lrt_pos","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals. ... Additional arguments pass metric_binomial function, citype type confidence interval method.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_lrt_pos.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Positive Likelihood Ratio — dx_lrt_pos","text":"Depending detail parameter, returns numeric value representing calculated metric data frame/tibble detailed diagnostics including confidence intervals possibly metrics relevant understanding metric.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_lrt_pos.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Positive Likelihood Ratio — dx_lrt_pos","text":"positive likelihood ratio calculated (TP / (TP + FN)) / (FP / (FP + TN)). used assess diagnostic usefulness test. LR+ much greater 1 indicates good diagnostic test can confidently confirm disease test positive.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_lrt_pos.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Positive Likelihood Ratio — dx_lrt_pos","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth,   threshold =     0.5, poslabel = 1 ) simple_lrp <- dx_lrt_pos(cm, detail = \"simple\") detailed_lrp <- dx_lrt_pos(cm) print(simple_lrp) #> [1] 8.700157 print(detailed_lrp) #> # A tibble: 1 × 8 #>   measure summary           estimate conf_low conf_high fraction conf_type notes #>   <chr>   <chr>                <dbl>    <dbl>     <dbl> <chr>    <chr>     <chr> #> 1 LRT+    8.70 (5.08, 14.9…     8.70     5.08      14.9 \"\"       Large sa… \"\""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_markedness.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Markedness — dx_markedness","title":"Calculate Markedness — dx_markedness","text":"Calculates Markedness provided confusion matrix. Markedness combined measure PPV (Positive Predictive Value) NPV (Negative Predictive Value). reflects effectiveness classifier marking class labels correctly, ranging -1 1.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_markedness.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Markedness — dx_markedness","text":"","code":"dx_markedness(cm, detail = \"full\", boot = FALSE, bootreps = 1000)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_markedness.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Markedness — dx_markedness","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals. boot Logical specifying confidence intervals generated via bootstrapping.  Note, can slow. bootreps number bootstrap replications calculating confidence intervals.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_markedness.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Markedness — dx_markedness","text":"Depending detail parameter, returns numeric value representing calculated metric data frame/tibble detailed diagnostics including confidence intervals possibly metrics relevant understanding metric.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_markedness.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Markedness — dx_markedness","text":"Markedness defined Markedness = PPV + NPV - 1. sum proportions predicted positives true positives (PPV) proportion predicted negatives true negatives (NPV) minus one. useful measure want consider positive negative predictive values test. higher markedness indicates better performance. formula Markedness : $$Markedness = PPV + NPV - 1$$","code":""},{"path":[]},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_markedness.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Markedness — dx_markedness","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth, threshold = 0.5, poslabel = 1) simple_markedness <- dx_markedness(cm, detail = \"simple\") detailed_markedness <- dx_markedness(cm) print(simple_markedness) #> [1] 0.6728395 print(detailed_markedness) #> # A tibble: 1 × 8 #>   measure    summary estimate conf_low conf_high fraction conf_type notes        #>   <chr>      <chr>      <dbl>    <dbl>     <dbl> <chr>    <chr>     <chr>        #> 1 Markedness 0.67       0.673       NA        NA \"\"       NA        Specify `bo…"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_mcc.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Matthews Correlation Coefficient (MCC) — dx_mcc","title":"Calculate Matthews Correlation Coefficient (MCC) — dx_mcc","text":"Calculates Matthews Correlation Coefficient (MCC), measure quality binary classifications. returns value -1 +1 +1 indicates perfect prediction, 0 better random prediction, -1 indicates total disagreement prediction observation. function can also return confidence interval MCC value using bootstrapping detail set \"full\".","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_mcc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Matthews Correlation Coefficient (MCC) — dx_mcc","text":"","code":"dx_mcc(cm, detail = \"full\", boot = FALSE, bootreps = 1000)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_mcc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Matthews Correlation Coefficient (MCC) — dx_mcc","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals. boot Logical specifying confidence intervals generated via bootstrapping.  Note, can slow. bootreps number bootstrap replications calculating confidence intervals.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_mcc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Matthews Correlation Coefficient (MCC) — dx_mcc","text":"detail \"simple\", returns single numeric value MCC. detail \"full\", returns data frame includes MCC, bootstrapped confidence interval, key details","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_mcc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Matthews Correlation Coefficient (MCC) — dx_mcc","text":"Matthews Correlation Coefficient used machine learning measure quality binary (two-class) classifications. takes account true false positives negatives generally regarded balanced measure can used even classes different sizes. formula MCC : $$MCC = \\frac{(TP \\times TN) - (FP \\times FN)}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}$$ TP, TN, FP, FN represent counts true positives, true negatives, false positives, false negatives, respectively. \"full\" details, bootstrap methods used estimate confidence interval MCC value, providing robust understanding stability.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_mcc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Matthews Correlation Coefficient (MCC) — dx_mcc","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth,   threshold =     0.5, poslabel = 1 ) mcc_simple <- dx_mcc(cm, detail = \"simple\") mcc_full <- dx_mcc(cm) print(mcc_simple) #> [1] 0.6428112 print(mcc_full) #> # A tibble: 1 × 8 #>   measure           summary estimate conf_low conf_high fraction conf_type notes #>   <chr>             <chr>      <dbl>    <dbl>     <dbl> <chr>    <chr>     <chr> #> 1 Matthews Correla… 0.64       0.643       NA        NA \"\"       NA        Spec…"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_mcnemars.html","id":null,"dir":"Reference","previous_headings":"","what":"McNemar's Chi-squared Test for Paired Proportions — dx_mcnemars","title":"McNemar's Chi-squared Test for Paired Proportions — dx_mcnemars","text":"Performs McNemar's test evaluate difference two paired proportions. typically used context binary classification test whether proportion correct classifications significantly differs two classifiers set instances.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_mcnemars.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"McNemar's Chi-squared Test for Paired Proportions — dx_mcnemars","text":"","code":"dx_mcnemars(dx1, dx2, detail = \"full\")"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_mcnemars.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"McNemar's Chi-squared Test for Paired Proportions — dx_mcnemars","text":"dx1 first dx object containing predictions truth values. dx2 second dx object containing predictions truth values. detail string indicating level detail output: \"simple\" just p-value, \"full\" comprehensive result including test statistics.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_mcnemars.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"McNemar's Chi-squared Test for Paired Proportions — dx_mcnemars","text":"Depending detail parameter, returns either p-value (simple) comprehensive list including test statistic p-value (full).","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_mcnemars.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"McNemar's Chi-squared Test for Paired Proportions — dx_mcnemars","text":"McNemar's test appropriate comparing classification results two algorithms data set (paired design). specifically tests null hypothesis marginal probabilities row column variable . test suitable binary classification tasks comparing performance two classifiers instances. appropriate independent samples two classifiers. function expects input two dx objects, containing predictions truth values classifiers compared. calculates contingency table based agreements disagreements classifiers applies McNemar's test table.","code":""},{"path":[]},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_mcnemars.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"McNemar's Chi-squared Test for Paired Proportions — dx_mcnemars","text":"","code":"dx_glm <- dx(data = dx_heart_failure, true_varname = \"truth\", pred_varname = \"predicted\") dx_rf <- dx(data = dx_heart_failure, true_varname = \"truth\", pred_varname = \"predicted_rf\") dx_mcnemars(dx_glm, dx_rf) #> # A tibble: 1 × 9 #>   models test        summary p_value estimate conf_low conf_high statistic notes #>   <chr>  <chr>       <chr>     <dbl> <chr>    <lgl>    <lgl>         <dbl> <chr> #> 1 \"\"     McNemar's … p=0.02   0.0180 \"\"       NA       NA              5.6 \"\""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_nir.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate No Information Rate (NIR) — dx_nir","title":"Calculate No Information Rate (NIR) — dx_nir","text":"Information Rate proportion largest class actual outcomes. represents accuracy naive model achieve always predicting frequent class. baseline measure classification performance.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_nir.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate No Information Rate (NIR) — dx_nir","text":"","code":"dx_nir(cm, detail = \"full\")"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_nir.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate No Information Rate (NIR) — dx_nir","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_nir.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate No Information Rate (NIR) — dx_nir","text":"Depending detail parameter, returns numeric value representing calculated metric data frame/tibble detailed diagnostics including confidence intervals possibly metrics relevant understanding metric.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_nir.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate No Information Rate (NIR) — dx_nir","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth, threshold = 0.5, poslabel = 1) nir <- dx_nir(cm) print(nir) #> # A tibble: 1 × 8 #>   measure           summary estimate conf_low conf_high fraction conf_type notes #>   <chr>             <chr>      <dbl> <lgl>    <lgl>     <chr>    <chr>     <chr> #> 1 No Information R… 0.62       0.625 NA       NA        163/261  \"\"        \"\""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_npv.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Negative Predictive Value (NPV) — dx_npv","title":"Calculate Negative Predictive Value (NPV) — dx_npv","text":"Calculates proportion true negatives total predicted negatives (true negatives + false negatives), known Negative Predictive Value (NPV). metric measure classifier's ability identify negatives correctly. Note NPV, like metrics, may fully represent classifier performance unbalanced datasets used alongside metrics.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_npv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Negative Predictive Value (NPV) — dx_npv","text":"","code":"dx_npv(cm, detail = \"full\", ...)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_npv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Negative Predictive Value (NPV) — dx_npv","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals. ... Additional arguments pass metric_binomial function, citype type confidence interval method.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_npv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Negative Predictive Value (NPV) — dx_npv","text":"Depending detail parameter, returns numeric value representing calculated metric data frame/tibble detailed diagnostics including confidence intervals possibly metrics relevant understanding metric.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_npv.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Negative Predictive Value (NPV) — dx_npv","text":"NPV ratio true negatives sum true false negatives. indicator well classifier can identify negative instances. High NPV means classifier reliable negative classifications. However, may influenced prevalence condition best used conjunction metrics like PPV, sensitivity, specificity comprehensive evaluation. formula NPV : $$NPV = \\frac{True Negatives}{True Negatives + False Negatives}$$","code":""},{"path":[]},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_npv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Negative Predictive Value (NPV) — dx_npv","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth,   threshold =     0.5, poslabel = 1 ) simple_npv <- dx_npv(cm, detail = \"simple\") detailed_npv <- dx_npv(cm) print(simple_npv) #> [1] 0.8333333 print(detailed_npv) #> # A tibble: 1 × 8 #>   measure           summary estimate conf_low conf_high fraction conf_type notes #>   <chr>             <chr>      <dbl>    <dbl>     <dbl> <chr>    <chr>     <chr> #> 1 Negative Predict… 83.3% …    0.833    0.771     0.885 150/180  Binomial… \"\""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_npv_prevalence.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Negative Predictive Value (NPV) at Target Prevalence — dx_npv_prevalence","title":"Calculate Negative Predictive Value (NPV) at Target Prevalence — dx_npv_prevalence","text":"Computes Negative Predictive Value (NPV) adjusted specified prevalence level. function useful understanding classifier performance settings actual prevalence condition may differ initial data.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_npv_prevalence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Negative Predictive Value (NPV) at Target Prevalence — dx_npv_prevalence","text":"","code":"dx_npv_prevalence(cm, prevalence, detail = \"full\", ...)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_npv_prevalence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Negative Predictive Value (NPV) at Target Prevalence — dx_npv_prevalence","text":"cm dx_cm object created dx_cm(). prevalence Numeric value 0 1, representing target prevalence adjusting NPV calculation. detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals. ... Additional arguments pass metric_binomial function, citype type confidence interval method.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_npv_prevalence.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Negative Predictive Value (NPV) at Target Prevalence — dx_npv_prevalence","text":"Depending detail parameter, returns numeric value representing calculated metric data frame/tibble detailed diagnostics including confidence intervals possibly metrics relevant understanding metric.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_npv_prevalence.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Negative Predictive Value (NPV) at Target Prevalence — dx_npv_prevalence","text":"function calculates NPV using formula: $$NPV = \\frac{Specificity \\times (1 - Prevalence)}{(1 - Sensitivity) \\times Prevalence + Specificity \\times (1 - Prevalence)}$$ Specificity true negative rate Sensitivity true positive rate. Adjusting NPV prevalence allows better estimation classifier’s performance different population settings. Confidence intervales calucated using simple logit (Mercaldo et al, 2007)","code":""},{"path":[]},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_npv_prevalence.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Negative Predictive Value (NPV) at Target Prevalence — dx_npv_prevalence","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth,   threshold = 0.5, poslabel = 1 ) # Calculate NPV at a prevalence of 0.1 dx_npv_prevalence(cm, prevalence = 0.1) #> # A tibble: 1 × 8 #>   measure           summary estimate conf_low conf_high fraction conf_type notes #>   <chr>             <chr>      <dbl>    <dbl>     <dbl> <chr>    <chr>     <chr> #> 1 Negative Predict… 96.4% …    0.964    0.952     0.973 \"\"       Simple L… Prev…"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_odds_ratio.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Odds Ratio — dx_odds_ratio","title":"Calculate Odds Ratio — dx_odds_ratio","text":"Calculates Odds Ratio () confusion matrix object. measure association exposure outcome. represents odds outcome occur given particular exposure, compared odds outcome occurring absence exposure.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_odds_ratio.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Odds Ratio — dx_odds_ratio","text":"","code":"dx_odds_ratio(cm, detail = \"full\", ...)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_odds_ratio.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Odds Ratio — dx_odds_ratio","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals. ... Additional arguments pass metric_binomial function, citype type confidence interval method.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_odds_ratio.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Odds Ratio — dx_odds_ratio","text":"Depending detail parameter, returns numeric value representing calculated metric data frame/tibble detailed diagnostics including confidence intervals possibly metrics relevant understanding metric.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_odds_ratio.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Odds Ratio — dx_odds_ratio","text":"odds ratio calculated (TP * TN) / (FP * FN). used case-control studies estimate strength association exposure outcome. Note value 1 indicates association, greater 1 indicates increased odds outcome exposure, less 1 indicates decreased odds.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_odds_ratio.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Odds Ratio — dx_odds_ratio","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth,   threshold =     0.5, poslabel = 1 ) simple_or <- dx_odds_ratio(cm, detail = \"simple\") detailed_or <- dx_odds_ratio(cm) print(simple_or) #> [1] 26.15385 print(detailed_or) #> # A tibble: 1 × 8 #>   measure    summary        estimate conf_low conf_high fraction conf_type notes #>   <chr>      <chr>             <dbl>    <dbl>     <dbl> <chr>    <chr>     <chr> #> 1 Odds Ratio 26.15 (12.84,…     26.2     12.8      53.3 \"\"       Large sa… \"\""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_calibration.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Calibration Curve — dx_plot_calibration","title":"Plot Calibration Curve — dx_plot_calibration","text":"Generates calibration plot assess calibration predicted probabilities observed outcomes. function plots mean predicted probability observed proportion positive outcomes different bins predicted probabilities.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_calibration.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Calibration Curve — dx_plot_calibration","text":"","code":"dx_plot_calibration(dx_obj, bins = 10)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_calibration.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Calibration Curve — dx_plot_calibration","text":"dx_obj list object containing model's predicted probabilities (predicted_probs) actual binary outcomes (true_labels). bins integer specifying number bins divide predicted probabilities . Default 10 (deciles).","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_calibration.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Calibration Curve — dx_plot_calibration","text":"ggplot object representing calibration curve, can customized needed.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_calibration.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot Calibration Curve — dx_plot_calibration","text":"calibration curve important diagnostic tool understand well model's predicted probabilities match observed frequencies outcomes. well-calibrated model calibration curve closely follows diagonal line y = x, indicating predicted probabilities reflective true likelihood event. curve constructed dividing predicted probabilities bins number bins plotting average predicted probability observed proportion positive outcomes bin. number bins can adjusted change granularity calibration curve. calibration curve deviates significantly diagonal line indicates model may overestimating probabilities. cases, calibration methods like isotonic regression Platt scaling might applied adjust model's predicted probabilities.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_calibration.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Calibration Curve — dx_plot_calibration","text":"","code":"dx_obj <- dx(   data = dx_heart_failure,   true_varname = \"truth\",   pred_varname = \"predicted\",   outcome_label = \"Heart Attack\",   setthreshold = .3 ) dx_plot_calibration(dx_obj)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_cap.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Cumulative Accuracy Profile (CAP) Curve — dx_plot_cap","title":"Plot Cumulative Accuracy Profile (CAP) Curve — dx_plot_cap","text":"Generates Cumulative Accuracy Profile (CAP) Curve evaluate visualize performance binary classification model comparing cumulative accuracy perfect random models.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_cap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Cumulative Accuracy Profile (CAP) Curve — dx_plot_cap","text":"","code":"dx_plot_cap(dx_obj)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_cap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Cumulative Accuracy Profile (CAP) Curve — dx_plot_cap","text":"dx_obj dx object containing rank-based statistics, including cumulative true positives percentiles. expected 'rank' element necessary data.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_cap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Cumulative Accuracy Profile (CAP) Curve — dx_plot_cap","text":"ggplot object representing CAP Curve actual, perfect, random model lines, allowing customization desired.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_cap.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot Cumulative Accuracy Profile (CAP) Curve — dx_plot_cap","text":"CAP Curve provides visual representation effectiveness binary classification model ranking positive instances compared two baseline strategies: perfect model captures positives immediately random model captures positives uniformly random. curve illustrates cumulative percentage captured positives (Y-axis) function cumulative percentage instances (X-axis). function plots three lines: Actual Model: Represents cumulative accuracy given model. Perfect Model: Represents hypothetical scenario positive instances ranked negative instances. Random Model: Represents baseline scenario randomly guessing class. CAP Curve useful understanding just accuracy also behavior model across entire range classifications. particularly insightful imbalanced datasets positive instances rare critical.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_cap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Cumulative Accuracy Profile (CAP) Curve — dx_plot_cap","text":"","code":"dx_obj <- dx(   data = dx_heart_failure,   true_varname = \"truth\",   pred_varname = \"predicted\",   outcome_label = \"Heart Attack\",   setthreshold = .3 ) dx_plot_cap(dx_obj)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_cm.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Confusion Matrix with Metrics — dx_plot_cm","title":"Plot Confusion Matrix with Metrics — dx_plot_cm","text":"Creates graphical representation confusion matrix dx object. plot displays counts true positives, false negatives, true negatives, false positives. Additionally, annotates plot relevant performance metrics Sensitivity, Specificity, PPV, NPV, along confidence intervals applicable.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_cm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Confusion Matrix with Metrics — dx_plot_cm","text":"","code":"dx_plot_cm(dx_obj, palette = c(\"#e5eef7\", \"#0057B8\"), levels = c(\"-\", \"+\"))"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_cm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Confusion Matrix with Metrics — dx_plot_cm","text":"dx_obj object class \"dx\" containing diagnostic measurements. palette character vector length 2 specifying colors low high ends fill gradient used plot. Defaults c(\"#e5eef7\", \"#0057B8\"). levels character vector length 2 specifying labels negative positive classes, respectively. Defaults c(\"-\", \"+\").","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_cm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Confusion Matrix with Metrics — dx_plot_cm","text":"ggplot object represents confusion matrix additional performance metrics.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_cm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Confusion Matrix with Metrics — dx_plot_cm","text":"","code":"dx_obj <- dx(   data = dx_heart_failure,   true_varname = \"truth\",   pred_varname = \"predicted\",   outcome_label = \"Heart Attack\",   setthreshold = .3 ) dx_plot_cm(dx_obj)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_cost.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Cost Curve — dx_plot_cost","title":"Plot Cost Curve — dx_plot_cost","text":"Generates plots cost curve binary classification model range thresholds, illustrating total cost associated different classification thresholds based specified costs false positives, false negatives, true positives, true negatives.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_cost.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Cost Curve — dx_plot_cost","text":"","code":"dx_plot_cost(dx_obj, cfp, cfn, ctp = 0, ctn = 0)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_cost.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Cost Curve — dx_plot_cost","text":"dx_obj dx object containing threshold-based statistics including false positives (fp) false negatives (fn) threshold. cfp Cost false positive. cfn Cost false negative. ctp Benefit (negative cost) true positive. Default 0. ctn Benefit (negative cost) true negative. Default 0.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_cost.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Cost Curve — dx_plot_cost","text":"ggplot object representing cost curve, displaying total cost y-axis classification threshold x-axis.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_cost.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot Cost Curve — dx_plot_cost","text":"cost curve represents total cost associated binary classification model classification threshold varied. calculated : $$Total Cost = CFP * FP + CFN * FN - CTP * TP - CTN * TN$$ FP, FN, TP, TN counts false positives, false negatives, true positives, true negatives threshold, respectively. curve helps identifying optimal threshold minimizes total cost maximizes overall benefit, considering specific cost/benefit structure different outcomes particular application. Typically, total cost vary depending threshold, reflecting trade-reducing one type error increasing another. adjusting threshold, users can determine point model provides best balance according specified cost matrix.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_cost.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Cost Curve — dx_plot_cost","text":"","code":"dx_obj <- dx(   data = dx_heart_failure,   true_varname = \"truth\",   pred_varname = \"predicted\",   outcome_label = \"Heart Attack\",   setthreshold = .3 ) dx_plot_cost(dx_obj, cfp = 1000, cfn = 8000)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_decision_curve.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Decision Curve — dx_plot_decision_curve","title":"Plot Decision Curve — dx_plot_decision_curve","text":"Generates decision curve visualize net benefit binary classification model across specified range threshold probabilities. decision curve shows net benefit using model different threshold probabilities compared strategy treating none.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_decision_curve.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Decision Curve — dx_plot_decision_curve","text":"","code":"dx_plot_decision_curve(dx_obj, plot_range = c(0.05, 0.95))"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_decision_curve.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Decision Curve — dx_plot_decision_curve","text":"dx_obj dx object containing 'thresholds' data frame columns 'threshold' 'net_benefit'. plot_range numeric vector length 2 specifying lower upper bounds range threshold probabilities plotted. Defaults c(0.05, .95).","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_decision_curve.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Decision Curve — dx_plot_decision_curve","text":"ggplot object representing decision curve, can customized needed.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_decision_curve.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot Decision Curve — dx_plot_decision_curve","text":"Decision Curve Analysis (DCA) method evaluating comparing clinical usefulness prediction models considering clinical consequences decision making. decision curve plots net benefit using model across range threshold probabilities making decision. net benefit calculated true positive rate (sensitivity) minus weighted false positive rate, weight ratio cost false positives relative benefit true positives. ratio derived threshold probability: $$Net Benefit = Sensitivity - (Weight * False Positive Rate)$$ Weight = Threshold / (1 - Threshold). decision curve typically compared two default strategies: treating patients treating none. net benefit higher model's predictions provide clear advantage default strategies. well-performing model decision curve stays treat-none treat-lines range clinically relevant threshold probabilities. 'plot_range' parameter allows limiting analysis relevant range thresholds, avoiding extremes net benefit calculation can become unstable clinically relevant.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_decision_curve.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Decision Curve — dx_plot_decision_curve","text":"","code":"dx_obj <- dx(   data = dx_heart_failure,   true_varname = \"truth\",   pred_varname = \"predicted\",   outcome_label = \"Heart Attack\",   setthreshold = .3 ) dx_plot_decision_curve(dx_obj)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_forest.html","id":null,"dir":"Reference","previous_headings":"","what":"Create table with odds ratios displayed graphically — dx_plot_forest","title":"Create table with odds ratios displayed graphically — dx_plot_forest","text":"Generate table diagnostic measures","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_forest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create table with odds ratios displayed graphically — dx_plot_forest","text":"","code":"dx_plot_forest(   dx_obj,   fraction = FALSE,   breaks = NA,   limits = NA,   tick_label_size = 6.5,   trans = c(NA, \"log10\"),   measures = c(\"AUC ROC\", \"Sensitivity\", \"Specificity\", \"Odds Ratio\"),   return = c(\"ggplot\", \"grid\"),   filename = NA,   header_bg = \"white\",   header_col = \"black\",   body_bg = c(\"#e6e4e2\", \"#ffffff\"),   footer_bg = \"#b8b6b4\",   footer_col = \"black\",   header_fontsize = 10,   body_fontsize = 9,   fraction_multiline = FALSE,   or_lwd = 0.8,   or_size = 0.35,   body_or_col = \"black\",   footer_or_col = footer_col )"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_forest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create table with odds ratios displayed graphically — dx_plot_forest","text":"dx_obj object class dx fraction Logical.  fraction sensitivity specificity shown? breaks numeric vector breaks include axis ticks. left empty, breaks determined using min max value 95% CIs. limits Limits axis ticks. Ticks generates using base::breaks. Ignored breaks passed. tick_label_size Font size axis labels. trans Method transform odds ratio . Currently, log10 supported. measures Measures included plot return grid ggplot object returned? filename File bane create disk. left NA, file created. header_bg Background color header header_col Color text header body_bg Background color table rows.  values less total number rows, values repeated. footer_bg Background color footer row. footer_col Color footer row. header_fontsize Font size header text body_fontsize Font size body text fraction_multiline Logical. fractions split onto 2 lines? or_lwd Line width or_size Size point body_or_col Color odds ratios table body footer_or_col Color odds ratios table footer","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_forest.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create table with odds ratios displayed graphically — dx_plot_forest","text":"","code":"dx_obj <- dx(   data = dx_heart_failure,   true_varname = \"truth\",   pred_varname = \"predicted\",   outcome_label = \"Heart Attack\",   threshold_range = c(.1, .2, .3),   setthreshold = .3,   grouping_variables = c(\"AgeGroup\", \"Sex\", \"AgeSex\") ) dx_plot_forest(dx_obj)  dx_plot_forest(dx_obj, trans = \"log10\")"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_gain.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Gain Chart — dx_plot_gain","title":"Plot Gain Chart — dx_plot_gain","text":"Constructs Gain chart dx object. Gain charts evaluative tool binary classification models, displaying effectiveness model identifying positive instances.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_gain.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Gain Chart — dx_plot_gain","text":"","code":"dx_plot_gain(dx_obj)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_gain.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Gain Chart — dx_plot_gain","text":"dx_obj dx object containing diagnostic measurements, including rank data frame percentile gain columns. object typically output diagnostic function computes various metrics evaluating model performance.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_gain.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Gain Chart — dx_plot_gain","text":"ggplot object representing Gain chart, allowing customization desired.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_gain.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot Gain Chart — dx_plot_gain","text":"Gain chart plots cumulative percentage true positive cases (gain) percentage cases evaluated model, ranked predicted probability positive. x-axis represents percentile population ordered model's predicted probability positive. y-axis represents cumulative percentage true positive cases found percentile. slope curve indicates model's ability prioritize positive cases negatives. steep curve towards top-left indicates model effectively ranks positive cases higher negatives, capturing large proportion positives early ranking. Conversely, curve close diagonal suggests performance close random chance, model effectively differentiate positive negative cases.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_gain.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Gain Chart — dx_plot_gain","text":"","code":"dx_obj <- dx(   data = dx_heart_failure,   true_varname = \"truth\",   pred_varname = \"predicted\",   outcome_label = \"Heart Attack\",   setthreshold = .3 ) dx_plot_gain(dx_obj)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_ks.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Kolmogorov-Smirnov Curve — dx_plot_ks","title":"Plot Kolmogorov-Smirnov Curve — dx_plot_ks","text":"Generates Kolmogorov-Smirnov (KS) plot dx object. KS plot graphical tool used assess discriminatory power binary classification model visualizing difference cumulative distribution functions true positive rate (TPR) false positive rate (FPR) across different thresholds.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_ks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Kolmogorov-Smirnov Curve — dx_plot_ks","text":"","code":"dx_plot_ks(dx_obj)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_ks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Kolmogorov-Smirnov Curve — dx_plot_ks","text":"dx_obj dx object containing diagnostic measurements, including rank data frame columns cumulative true positive rate (TPR) false positive rate (FPR), well predicted probabilities associated instance.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_ks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Kolmogorov-Smirnov Curve — dx_plot_ks","text":"ggplot object representing KS plot, can customized needed.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_ks.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot Kolmogorov-Smirnov Curve — dx_plot_ks","text":"KS plot displays two lines representing cumulative TPR FPR, x-axis showing prediction scores descending order y-axis showing cumulative proportion positive negative instances. point difference TPR FPR maximal indicates threshold highest potential separating positive negative instances. point marked dashed vertical line plot, KS statistic, representing maximum distance two lines, annotated plot. higher KS statistic indicates model better discriminatory ability. ideal model KS plot TPR line close top-left corner FPR line close bottom-right corner, maximizing distance two lines.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_ks.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Kolmogorov-Smirnov Curve — dx_plot_ks","text":"","code":"dx_obj <- dx(   data = dx_heart_failure,   true_varname = \"truth\",   pred_varname = \"predicted\",   outcome_label = \"Heart Attack\",   setthreshold = .3 ) dx_plot_ks(dx_obj)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_lift.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Lift Curve — dx_plot_lift","title":"Plot Lift Curve — dx_plot_lift","text":"Generates Lift chart dx object. Lift charts used evaluate performance binary classification models comparing results using model versus random selection. Lift chart plots ratio results obtained model obtained random model, across different percentiles population.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_lift.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Lift Curve — dx_plot_lift","text":"","code":"dx_plot_lift(dx_obj)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_lift.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Lift Curve — dx_plot_lift","text":"dx_obj dx object containing diagnostic measurements, including rank data frame percentile lift columns. rank data frame result diagnostic process scores ranks instance based likelihood true positive.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_lift.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Lift Curve — dx_plot_lift","text":"ggplot object representing Lift chart, can customized needed.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_lift.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot Lift Curve — dx_plot_lift","text":"Lift chart visualizes much likely capture positive instances using model's predictions compared random guess. x-axis represents percentile population ordered predicted probabilities, y-axis represents lift, calculated ratio cumulative gain percentile gain expected chance. value greater 1 indicates model performing better random, higher values representing better performance. horizontal dashed line y=1 represents baseline lift random model. lift curve ideally stay line indicate model predictive power.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_lift.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Lift Curve — dx_plot_lift","text":"","code":"dx_obj <- dx(   data = dx_heart_failure,   true_varname = \"truth\",   pred_varname = \"predicted\",   outcome_label = \"Heart Attack\",   setthreshold = .3 ) dx_plot_lift(dx_obj)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_pr.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Precision-Recall Curve — dx_plot_pr","title":"Plot Precision-Recall Curve — dx_plot_pr","text":"Plots Precision-Recall (PR) curve dx object. PR curve useful tool evaluating performance binary classification model, especially situations classes imbalanced. shows trade-precision recall different thresholds.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_pr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Precision-Recall Curve — dx_plot_pr","text":"","code":"dx_plot_pr(dx_obj)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_pr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Precision-Recall Curve — dx_plot_pr","text":"dx_obj dx object containing diagnostic measurements, including data frame sensitivity (recall) precision values various thresholds.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_pr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Precision-Recall Curve — dx_plot_pr","text":"ggplot object representing PR curve, can customized needed.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_pr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot Precision-Recall Curve — dx_plot_pr","text":"PR curve plot demonstrates relationship precision (proportion true positive predictions among positive predictions) recall (proportion true positive predictions among actual positives), possible cut-threshold. model perfect performance show line reaches top-right corner plot, indicating high precision recall. area PR curve (AUPRC) can also used summary measure model performance, higher values indicating better performance. function generates curve without calculating area; separate functions used AUPRC required.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_pr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Precision-Recall Curve — dx_plot_pr","text":"","code":"dx_obj <- dx(   data = dx_heart_failure,   true_varname = \"truth\",   pred_varname = \"predicted\",   outcome_label = \"Heart Attack\",   setthreshold = .3 ) dx_plot_pr(dx_obj)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_predictive_value.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Predictive Values Against Prevalence — dx_plot_predictive_value","title":"Plot Predictive Values Against Prevalence — dx_plot_predictive_value","text":"Generates plot Positive Predictive Value (PPV) Negative Predictive Value (NPV) across range disease prevalences. plot helps understanding PPV NPV diagnostic test vary prevalence condition.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_predictive_value.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Predictive Values Against Prevalence — dx_plot_predictive_value","text":"","code":"dx_plot_predictive_value(dx_obj)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_predictive_value.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Predictive Values Against Prevalence — dx_plot_predictive_value","text":"dx_obj dx object containing prevalence analysis data, including calculated PPV NPV various prevalence levels.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_predictive_value.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Predictive Values Against Prevalence — dx_plot_predictive_value","text":"ggplot object representing Predictive Value Plot.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_predictive_value.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot Predictive Values Against Prevalence — dx_plot_predictive_value","text":"Predictive Value Plot visualizes Positive Predictive Value (PPV) Negative Predictive Value (NPV) test change varying disease prevalence. Typically, prevalence increases, PPV increases NPV decreases. disease common (higher prevalence), positive test result likely true positive, thus increasing PPV. Conversely, disease less common (lower prevalence), negative test result likely true negative, increasing NPV. impact prevalence PPV NPV fundamental concept medical testing, understanding population's disease prevalence crucial interpreting test results. rare conditions, even tests high sensitivity specificity can low PPV, meaning positive results false positives. Similarly, common conditions, NPV can decrease, indicating negative results become less reliable. plot helps visualizing relationships valuable tool evaluation diagnostic tests, allowing healthcare professionals researchers anticipate well test perform different scenarios.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_predictive_value.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Predictive Values Against Prevalence — dx_plot_predictive_value","text":"","code":"dx_obj <- dx(   data = dx_heart_failure,   true_varname = \"truth\",   pred_varname = \"predicted\",   outcome_label = \"Heart Attack\",   setthreshold = .3 ) dx_plot_predictive_value(dx_obj)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_probabilities.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Predicted Probabilities — dx_plot_probabilities","title":"Plot Predicted Probabilities — dx_plot_probabilities","text":"Generates plot showing distribution predicted probabilities. Offers options show histogram density plot.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_probabilities.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Predicted Probabilities — dx_plot_probabilities","text":"","code":"dx_plot_probabilities(   dx_obj,   plot_type = \"histogram\",   bins = NULL,   fill_color = \"#0057B8\" )"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_probabilities.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Predicted Probabilities — dx_plot_probabilities","text":"dx_obj dx object plot_type Character string specifying type plot: \"histogram\" \"density\". bins Optional; number bins histogram (relevant plot_type = \"histogram\"). fill_color Color fill bars density plot.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_probabilities.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Predicted Probabilities — dx_plot_probabilities","text":"ggplot object representing distribution predicted probabilities.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_probabilities.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Predicted Probabilities — dx_plot_probabilities","text":"","code":"dx_obj <- dx(   data = dx_heart_failure,   true_varname = \"truth\",   pred_varname = \"predicted\",   outcome_label = \"Heart Attack\",   setthreshold = .3 ) dx_plot_probabilities(dx_obj, plot_type = \"histogram\", bins = 30)  dx_plot_probabilities(dx_obj, plot_type = \"density\")"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_roc.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot ROC Curve — dx_plot_roc","title":"Plot ROC Curve — dx_plot_roc","text":"Generates plots Receiver Operating Characteristic (ROC) curve binary classification model represented given dx object. ROC curve graphical representation trade-true positive rate (sensitivity) false positive rate (1 - specificity) various threshold settings.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_roc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot ROC Curve — dx_plot_roc","text":"","code":"dx_plot_roc(   dx_obj,   curve_color = \"#0057B8\",   fill_color = \"#cfcdcb\",   text_color = \"black\",   add_text = TRUE,   add_ref_lines = TRUE,   add_fractions = TRUE,   axis_color = \"#333333\",   add_ref_circle = TRUE,   ref_lines_color = \"#8a8887\",   circle_ref_color = \"#E4002B\",   summary_stats = c(1, 2, 3, 4, 5, 6, 7, 8),   filename = NA )"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_roc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot ROC Curve — dx_plot_roc","text":"dx_obj object class dx containing necessary data statistics generating ROC curve. curve_color Color ROC curve. Default \"#0057B8\". fill_color Color filled ROC curve. Use \"transparent\" fill. Default \"#cfcdcb\". text_color Color text included ROC curve. Default \"black\". add_text Logical; TRUE, includes statistical annotations ROC curve. Default TRUE. add_ref_lines Logical; TRUE, includes reference lines ROC curve. Default TRUE. add_fractions Logical; TRUE, includes fraction details text annotations. Default TRUE. axis_color Color x y axis. Default \"#333333\". add_ref_circle Logical; TRUE, includes reference circle around point specified threshold. Default TRUE. ref_lines_color Color reference lines. Default \"#8a8887\". circle_ref_color Color reference circle. Default \"#E4002B\". summary_stats vector integers indicating statistics include ROC curve. Default c(1, 2, 3, 4, 5, 6, 7, 8). filename File name create disk using ggplot2::ggsave. left NA, file created.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_roc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot ROC Curve — dx_plot_roc","text":"ggplot object representing ROC curve, allowing customization desired.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_roc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot ROC Curve — dx_plot_roc","text":"ROC curve widely used tool diagnosing performance binary classification models. plots true positive rate (sensitivity) false positive rate (1 - specificity) various thresholds. model perfect discrimination (overlap two distributions) ROC curve passes upper left corner (100% sensitivity, 100% specificity). Therefore closer ROC curve upper left corner, higher overall accuracy test. dx_roc function allows extensive customization ROC curve, including color schemes, reference lines, text annotations, , accommodate variety visualization needs preferences. area ROC curve (AUC) provides single scalar value summarizing performance test. AUC can interpreted probability classifier rank randomly chosen positive instance higher randomly chosen negative instance. function also provides options include reference circle specific threshold point, reference lines indicating 45-degree line (chance line) lines chosen threshold's specificity sensitivity, fill color curve.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_roc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot ROC Curve — dx_plot_roc","text":"","code":"dx_obj <- dx(   data = dx_heart_failure,   true_varname = \"truth\",   pred_varname = \"predicted\",   outcome_label = \"Heart Attack\",   threshold_range = c(.1, .2, .3),   setthreshold = .3,   grouping_variables = c(\"AgeGroup\", \"Sex\", \"AgeSex\") ) dx_plot_roc(dx_obj) #> Warning: All aesthetics have length 1, but the data has 263 rows. #> ℹ Please consider using `annotate()` or provide this layer with data containing #>   a single row. #> Warning: All aesthetics have length 1, but the data has 263 rows. #> ℹ Please consider using `annotate()` or provide this layer with data containing #>   a single row. #> Warning: All aesthetics have length 1, but the data has 263 rows. #> ℹ Please consider using `annotate()` or provide this layer with data containing #>   a single row. #> Warning: All aesthetics have length 1, but the data has 263 rows. #> ℹ Please consider using `annotate()` or provide this layer with data containing #>   a single row."},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_rocs.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot ROC Curves for Multiple Models — dx_plot_rocs","title":"Plot ROC Curves for Multiple Models — dx_plot_rocs","text":"Generates Receiver Operating Characteristic (ROC) curves multiple models overlays comparison. Optionally, adds text annotations DeLong's test results indicate statistical differences models' Area ROC Curve (AUC).","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_rocs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot ROC Curves for Multiple Models — dx_plot_rocs","text":"","code":"dx_plot_rocs(   dx_comp,   add_text = TRUE,   axis_color = \"#333333\",   text_color = \"black\" )"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_rocs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot ROC Curves for Multiple Models — dx_plot_rocs","text":"dx_comp dx_compare object containing results pairwise model comparisons list dx objects ROC data. add_text Logical, whether add DeLong's test results text annotations plot. Defaults TRUE. axis_color Color axes lines, specified color name hex code. Defaults \"#333333\". text_color Color text annotations, specified color name hex code. Defaults \"black\".","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_rocs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot ROC Curves for Multiple Models — dx_plot_rocs","text":"ggplot object representing ROC curves models included dx_comp object. model's ROC curve color-coded, plot includes annotations DeLong's test p-values add_text TRUE.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_rocs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot ROC Curves for Multiple Models — dx_plot_rocs","text":"function visualization tool plots ROC curves multiple models facilitate comparison. uses DeLong's test statistically compare AUC values , desired, annotates plot results. function expects dx_compare object input, contain necessary ROC test comparison data. Ensure ROC data DeLong's test results appropriately generated stored dx_compare object using function.","code":""},{"path":[]},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_rocs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot ROC Curves for Multiple Models — dx_plot_rocs","text":"","code":"dx_glm <- dx(data = dx_heart_failure, true_varname = \"truth\", pred_varname = \"predicted\") dx_rf <- dx(data = dx_heart_failure, true_varname = \"truth\", pred_varname = \"predicted_rf\") dx_list <- list(dx_glm, dx_rf) dx_comp <- dx_compare(dx_list, paired = TRUE) dx_plot_rocs(dx_comp)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_thresholds.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Diagnostic Measures across Thresholds — dx_plot_thresholds","title":"Plot Diagnostic Measures across Thresholds — dx_plot_thresholds","text":"Generates line plot visualize various diagnostic measures across different threshold values binary classification model. visualization can help selecting optimal threshold based trade-offs different measures.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_thresholds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Diagnostic Measures across Thresholds — dx_plot_thresholds","text":"","code":"dx_plot_thresholds(dx_obj)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_thresholds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Diagnostic Measures across Thresholds — dx_plot_thresholds","text":"dx_obj dx object containing threshold-based statistics, including values various diagnostic measures different thresholds.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_thresholds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Diagnostic Measures across Thresholds — dx_plot_thresholds","text":"ggplot object representing diagnostic measures across thresholds ability customize desired.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_thresholds.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot Diagnostic Measures across Thresholds — dx_plot_thresholds","text":"function plots multiple lines representing different diagnostic measures NPV, PPV, sensitivity, specificity, F1 score across range threshold values. line corresponds specific metric, illustrating measure changes classification threshold varied. vertical dashed line indicates set threshold dx object reference. plot particularly useful understanding behavior classifier different operating conditions identifying threshold balances trade-offs various measures according specific needs application.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_thresholds.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Diagnostic Measures across Thresholds — dx_plot_thresholds","text":"","code":"dx_obj <- dx(   data = dx_heart_failure,   true_varname = \"truth\",   pred_varname = \"predicted\",   outcome_label = \"Heart Attack\",   setthreshold = .3 ) dx_plot_thresholds(dx_obj)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_youden_j.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Youden's J Index Curve — dx_plot_youden_j","title":"Plot Youden's J Index Curve — dx_plot_youden_j","text":"Generates plot Youden's J Index (Informedness) across range threshold probabilities binary classification model. curve shows trade-sensitivity specificity varies threshold.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_youden_j.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Youden's J Index Curve — dx_plot_youden_j","text":"","code":"dx_plot_youden_j(dx_obj)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_youden_j.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Youden's J Index Curve — dx_plot_youden_j","text":"dx_obj dx object containing 'thresholds' data frame columns 'threshold' 'informedness'.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_youden_j.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Youden's J Index Curve — dx_plot_youden_j","text":"ggplot object representing Youden's J Index curve, can customized needed.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_youden_j.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot Youden's J Index Curve — dx_plot_youden_j","text":"Youden's J Index summary measure diagnostic effectiveness biomarker test, defined maximum vertical distance ROC curve diagonal line. calculated \\(J = Sensitivity + Specificity - 1\\), higher Youden's J Index indicates better test performance better trade-sensitivity specificity. index varies -1 1, 0 indicates better performance random guessing 1 indicates perfect performance. curve plots Youden's J Index different threshold probabilities, enabling visualization model performance (terms balance sensitivity specificity) changes across thresholds. peak curve indicates threshold optimal balance sensitivity specificity according Youden's J Index.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_plot_youden_j.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Youden's J Index Curve — dx_plot_youden_j","text":"","code":"dx_obj <- dx(   data = dx_heart_failure,   true_varname = \"truth\",   pred_varname = \"predicted\",   outcome_label = \"Heart Attack\",   setthreshold = .3 ) dx_plot_youden_j(dx_obj)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_ppv_prevalence.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Positive Predictive Value (PPV) at Target Prevalence — dx_ppv_prevalence","title":"Calculate Positive Predictive Value (PPV) at Target Prevalence — dx_ppv_prevalence","text":"Computes Positive Predictive Value (PPV) adjusted specified prevalence level. function useful understanding classifier performance settings actual prevalence condition may differ initial data.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_ppv_prevalence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Positive Predictive Value (PPV) at Target Prevalence — dx_ppv_prevalence","text":"","code":"dx_ppv_prevalence(cm, prevalence, detail = \"full\", ...)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_ppv_prevalence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Positive Predictive Value (PPV) at Target Prevalence — dx_ppv_prevalence","text":"cm dx_cm object created dx_cm(). prevalence Numeric value 0 1, representing target prevalence adjusting PPV calculation. detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals. ... Additional arguments pass metric_binomial function, citype type confidence interval method.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_ppv_prevalence.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Positive Predictive Value (PPV) at Target Prevalence — dx_ppv_prevalence","text":"Depending detail parameter, returns numeric value representing calculated metric data frame/tibble detailed diagnostics including confidence intervals possibly metrics relevant understanding metric.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_ppv_prevalence.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Positive Predictive Value (PPV) at Target Prevalence — dx_ppv_prevalence","text":"function calculates PPV using formula: $$PPV = \\frac{Sensitivity \\times Prevalence}{(Sensitivity \\times Prevalence) + (1 - Specificity) \\times (1 - Prevalence)}$$ Sensitivity true positive rate Specificity true negative rate. Adjusting PPV prevalence allows accurate assessment classifier’s performance different population settings. Confidence intervales calucated using simple logit (Mercaldo et al, 2007)","code":""},{"path":[]},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_ppv_prevalence.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Positive Predictive Value (PPV) at Target Prevalence — dx_ppv_prevalence","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth,   threshold = 0.5, poslabel = 1 ) # Calculate PPV at a prevalence of 0.1 dx_ppv_prevalence(cm, prevalence = 0.1) #> # A tibble: 1 × 8 #>   measure           summary estimate conf_low conf_high fraction conf_type notes #>   <chr>             <chr>      <dbl>    <dbl>     <dbl> <chr>    <chr>     <chr> #> 1 Positive Predict… 49.2% …    0.492    0.361     0.623 \"\"       Simple L… Prev…"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_prevalence.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Prevalence — dx_prevalence","title":"Calculate Prevalence — dx_prevalence","text":"Calculates Prevalence, proportion cases positive condition interest total number cases. Prevalence provides measure widespread condition within population given time.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_prevalence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Prevalence — dx_prevalence","text":"","code":"dx_prevalence(cm, detail = \"full\", ...)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_prevalence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Prevalence — dx_prevalence","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals. ... Additional arguments pass metric_binomial function, citype type confidence interval method.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_prevalence.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Prevalence — dx_prevalence","text":"Depending detail parameter, returns numeric value representing calculated metric data frame/tibble detailed diagnostics including confidence intervals possibly metrics relevant understanding metric.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_prevalence.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Prevalence — dx_prevalence","text":"Prevalence measure burden condition disease population. important measure epidemiology health service planning helps understand level disease population given time. Unlike metrics based classifier's performance, prevalence measure actual condition tested. formula Prevalence : $$Prevalence = \\frac{Number Current Cases (Positives)}{Total Number Cases}$$","code":""},{"path":[]},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_prevalence.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Prevalence — dx_prevalence","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth,   threshold =     0.5, poslabel = 1 ) simple_prevalence <- dx_prevalence(cm, detail = \"simple\") detailed_prevalence <- dx_prevalence(cm) print(simple_prevalence) #> [1] 0.3754789 print(detailed_prevalence) #> # A tibble: 1 × 8 #>   measure    summary        estimate conf_low conf_high fraction conf_type notes #>   <chr>      <chr>             <dbl>    <dbl>     <dbl> <chr>    <chr>     <chr> #> 1 Prevalence 37.5% (31.7%,…    0.375    0.317     0.437 98/261   Binomial… \"\""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_roc_ggtheme.html","id":null,"dir":"Reference","previous_headings":"","what":"Theme used for dx_roc — dx_roc_ggtheme","title":"Theme used for dx_roc — dx_roc_ggtheme","text":"Theme used dx_roc","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_roc_ggtheme.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Theme used for dx_roc — dx_roc_ggtheme","text":"","code":"dx_roc_ggtheme()"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_z_test.html","id":null,"dir":"Reference","previous_headings":"","what":"Z-test for Comparing Two Proportions — dx_z_test","title":"Z-test for Comparing Two Proportions — dx_z_test","text":"Conducts two-sided Z-test (using prop.test two proportions) assess whether success rates (proportions) two groups different based specified metric. can compare accuracy, PPV, NPV, FNR, FPR, FDR, sensitivity, specificity two dx objects.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_z_test.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Z-test for Comparing Two Proportions — dx_z_test","text":"","code":"dx_z_test(   dx1,   dx2,   metric = c(\"accuracy\", \"ppv\", \"npv\", \"fnr\", \"fpr\", \"fdr\", \"sensitivity\", \"specificity\"),   detail = \"full\" )"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_z_test.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Z-test for Comparing Two Proportions — dx_z_test","text":"dx1 dx object first group. dx2 dx object second group. metric character string specifying metric compare two groups. Options include \"accuracy\", \"ppv\", \"npv\", \"fnr\", \"fpr\", \"fdr\", \"sensitivity\", \"specificity\". detail Character specifying level detail output: \"simple\" raw estimate (p-value ), \"full\" detailed estimate including confidence intervals test statistic.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_z_test.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Z-test for Comparing Two Proportions — dx_z_test","text":"Depending detail parameter, returns p-value test detailed list including test statistic, confidence interval, p-value.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_z_test.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Z-test for Comparing Two Proportions — dx_z_test","text":"function uses prop.test function perform hypothesis test, assuming null hypothesis two proportions based specified metric . low p-value indicates significant difference proportions, suggesting success rates two groups statistically significantly different. function automatically adjusts continuity provides confidence intervals difference proportions. Z-test two proportions appropriate comparing success rates (proportions) two independent samples. considerations using test: Independence: samples independent. test appropriate paired matched data. Sample Size**: groups sufficiently large number trials. rule thumb, test assumes number successes failures group least 5. However, accurate results, especially cases extreme proportions (close 0 1), larger sample sizes may necessary. Binary Outcome: test specific binary outcomes (success/failure, presence/absence, etc.) represented proportions. suitable continuous data counts converted proportions. Normal Approximation: Z-test based normal approximation distribution sample proportion. approximation accurate sample size large proportion extremely close 0 1. also important note prop.test adjusts continuity, adjustment may sufficient small sample sizes unbalanced designs. Always consider context assumptions data interpreting results test.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/dx_z_test.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Z-test for Comparing Two Proportions — dx_z_test","text":"","code":"dx_glm <- dx(data = dx_heart_failure, true_varname = \"truth\", pred_varname = \"predicted\") dx_rf <- dx(data = dx_heart_failure, true_varname = \"truth\", pred_varname = \"predicted_rf\") dx_z_test(dx_glm, dx_rf, metric = \"accuracy\") #> # A tibble: 1 × 9 #>   models test        summary p_value estimate conf_low conf_high statistic notes #>   <chr>  <chr>       <chr>     <dbl>    <dbl>    <dbl>     <dbl>     <dbl> <chr> #> 1 \"\"     Z-test: ac… 0.06 (…   0.121   0.0575  -0.0140     0.129      2.41 \"\""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/expected_cell_count.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Expected Cell Count — expected_cell_count","title":"Calculate Expected Cell Count — expected_cell_count","text":"Calculates expected count top left cell () null hypothesis homogeneity odds ratios across strata.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/expected_cell_count.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Expected Cell Count — expected_cell_count","text":"","code":"expected_cell_count(a_value, marginals, odds_ratio)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/expected_cell_count.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Expected Cell Count — expected_cell_count","text":"a_value observed count cell '' single stratum. marginals list containing 'row' 'col' marginal sums stratum. odds_ratio overall odds ratio use calculation.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/expected_cell_count.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Expected Cell Count — expected_cell_count","text":"expected count cell ''.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/fnr.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate False Negative Rate (FNR) — fnr","title":"Calculate False Negative Rate (FNR) — fnr","text":"Calculates False Negative Rate (FNR), proportion actual positives incorrectly identified negatives classifier. FNR also known miss rate critical measure evaluating performance classifier, especially scenarios failing detect positives costly.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/fnr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate False Negative Rate (FNR) — fnr","text":"","code":"dx_fnr(cm, detail = \"full\", ...)  dx_miss_rate(cm, detail = \"full\", ...)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/fnr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate False Negative Rate (FNR) — fnr","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals. ... Additional arguments pass metric_binomial function, citype type confidence interval method.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/fnr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate False Negative Rate (FNR) — fnr","text":"Depending detail parameter, returns numeric value representing calculated metric data frame/tibble detailed diagnostics including confidence intervals possibly metrics relevant understanding metric.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/fnr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate False Negative Rate (FNR) — fnr","text":"FNR important measure situations cost missing positive classification high. complements True Positive Rate (sensitivity) helps understanding trade-offs identifying positives avoiding false alarms. lower FNR generally desirable indicates sensitive classifier. formula FNR : $$FNR = \\frac{False Negatives}{False Negatives + True Positives}$$","code":""},{"path":[]},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/fnr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate False Negative Rate (FNR) — fnr","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth,   threshold =     0.5, poslabel = 1 ) simple_fnr <- dx_fnr(cm, detail = \"simple\") detailed_fnr <- dx_fnr(cm) print(simple_fnr) #> [1] 0.3061224 print(detailed_fnr) #> # A tibble: 1 × 8 #>   measure           summary estimate conf_low conf_high fraction conf_type notes #>   <chr>             <chr>      <dbl>    <dbl>     <dbl> <chr>    <chr>     <chr> #> 1 False Negative R… 30.6% …    0.306    0.217     0.407 30/98    Binomial… \"\""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/fpr.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate False Positive Rate (FPR) — fpr","title":"Calculate False Positive Rate (FPR) — fpr","text":"Calculates False Positive Rate (FPR), proportion actual negatives incorrectly identified positives classifier. FPR also known fall-rate crucial evaluating specificity classifier.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/fpr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate False Positive Rate (FPR) — fpr","text":"","code":"dx_fpr(cm, detail = \"full\", ...)  dx_fall_out(cm, detail = \"full\", ...)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/fpr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate False Positive Rate (FPR) — fpr","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals. ... Additional arguments pass metric_binomial function, citype type confidence interval method.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/fpr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate False Positive Rate (FPR) — fpr","text":"Depending detail parameter, returns numeric value representing calculated metric data frame/tibble detailed diagnostics including confidence intervals possibly metrics relevant understanding metric.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/fpr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate False Positive Rate (FPR) — fpr","text":"FPR particularly important contexts false alarms costly. used alongside True Negative Rate (specificity) understand classifier's ability correctly identify negative instances. lower FPR indicates classifier better correctly identifying negatives alarming false positives. formula FPR : $$FPR = \\frac{False Positives}{False Positives + True Negatives}$$","code":""},{"path":[]},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/fpr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate False Positive Rate (FPR) — fpr","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth,   threshold =     0.5, poslabel = 1 ) simple_fpr <- dx_fpr(cm, detail = \"simple\") detailed_fpr <- dx_fpr(cm) print(simple_fpr) #> [1] 0.0797546 print(detailed_fpr) #> # A tibble: 1 × 8 #>   measure           summary estimate conf_low conf_high fraction conf_type notes #>   <chr>             <chr>      <dbl>    <dbl>     <dbl> <chr>    <chr>     <chr> #> 1 False Positive R… 8.0% (…   0.0798   0.0431     0.133 13/163   Binomial… \"\""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/get_roc.html","id":null,"dir":"Reference","previous_headings":"","what":"Return an pROC::roc object for a dx object — get_roc","title":"Return an pROC::roc object for a dx object — get_roc","text":"Return pROC::roc object dx object","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/get_roc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return an pROC::roc object for a dx object — get_roc","text":"","code":"get_roc(true_varname, pred_varname, data, direction)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/get_roc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return an pROC::roc object for a dx object — get_roc","text":"true_varname Column name containing AI reference standard (string) pred_varname Column name containing AI prediction (string) data tbl. direction Direction roc comparison.  See ?pROC::roc","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/informedness.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Informedness — informedness","title":"Calculate Informedness — informedness","text":"Calculates Informedness provided confusion matrix. Informedness combined measure Sensitivity (True Positive Rate) Specificity (True Negative Rate). reflects probability classifier informed true class, ranging -1 1.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/informedness.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Informedness — informedness","text":"","code":"dx_informedness(cm, detail = \"full\", boot = FALSE, bootreps = 1000)  dx_youden_j(cm, detail = \"full\", boot = FALSE, bootreps = 1000)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/informedness.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Informedness — informedness","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals. boot Logical specifying confidence intervals generated via bootstrapping.  Note, can slow. bootreps number bootstrap replications calculating confidence intervals.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/informedness.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Informedness — informedness","text":"Depending detail parameter, returns numeric value representing calculated metric data frame/tibble detailed diagnostics including confidence intervals possibly metrics relevant understanding metric.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/informedness.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Informedness — informedness","text":"Informedness defined Informedness = Sensitivity + Specificity - 1. sum true positive rate true negative rate minus one. useful measure want consider sensitivity specificity test. higher informedness indicates better overall performance classifier distinguishing classes. formula Informedness : $$Informedness = Sensitivity + Specificity - 1$$","code":""},{"path":[]},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/informedness.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Informedness — informedness","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth, threshold = 0.5, poslabel = 1) simple_informedness <- dx_informedness(cm, detail = \"simple\") detailed_informedness <- dx_informedness(cm) print(simple_informedness) #> [1] 0.6141229 print(detailed_informedness) #> # A tibble: 1 × 8 #>   measure      summary estimate conf_low conf_high fraction conf_type notes      #>   <chr>        <chr>      <dbl>    <dbl>     <dbl> <chr>    <chr>     <chr>      #> 1 Informedness 0.61       0.614       NA        NA \"\"       NA        Specify `…"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/margin_sums.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Margin Sums for a Contingency Table — margin_sums","title":"Calculate Margin Sums for a Contingency Table — margin_sums","text":"Calculates row column sums 2x2 stratum contingency table.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/margin_sums.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Margin Sums for a Contingency Table — margin_sums","text":"","code":"margin_sums(stratum_data)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/margin_sums.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Margin Sums for a Contingency Table — margin_sums","text":"stratum_data 2x2 matrix representing single stratum contingency table.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/margin_sums.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Margin Sums for a Contingency Table — margin_sums","text":"list 'row' 'col' elements containing sums rows columns, respectively.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/metrics-params.html","id":null,"dir":"Reference","previous_headings":"","what":"Shared Parameters for Diagnostic Metrics — metrics-params","title":"Shared Parameters for Diagnostic Metrics — metrics-params","text":"Shared Parameters Diagnostic Metrics","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/metrics-params.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Shared Parameters for Diagnostic Metrics — metrics-params","text":"","code":"metricparams()"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/metrics-params.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Shared Parameters for Diagnostic Metrics — metrics-params","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals. ... Additional arguments pass metric_binomial function, citype type confidence interval method. boot Logical specifying confidence intervals generated via bootstrapping.  Note, can slow. bootreps number bootstrap replications calculating confidence intervals. predprob Numeric vector predicted probabilities associated positive class. truth Numeric vector true binary outcomes, typically 0 1, length predprob. dx1 dx object dx2 dx object","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/metrics-params.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Shared Parameters for Diagnostic Metrics — metrics-params","text":"Depending detail parameter, returns numeric value representing calculated metric data frame/tibble detailed diagnostics including confidence intervals possibly metrics relevant understanding metric.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/ppv.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Positive Predictive Value (PPV, Precision) — ppv","title":"Calculate Positive Predictive Value (PPV, Precision) — ppv","text":"Calculates proportion true positives total predicted positives (true positives + false positives).  PPV also known precision.Note PPV can influenced prevalence condition used alongside metrics.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/ppv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Positive Predictive Value (PPV, Precision) — ppv","text":"","code":"dx_ppv(cm, detail = \"full\", ...)  dx_precision(cm, detail = \"full\", ...)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/ppv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Positive Predictive Value (PPV, Precision) — ppv","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals. ... Additional arguments pass metric_binomial function, citype type confidence interval method.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/ppv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Positive Predictive Value (PPV, Precision) — ppv","text":"Depending detail parameter, returns numeric value representing calculated metric data frame/tibble detailed diagnostics including confidence intervals possibly metrics relevant understanding metric.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/ppv.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Positive Predictive Value (PPV, Precision) — ppv","text":"PPV, also known precision, ratio true positives sum true false positives. reflects classifier's ability identify relevant instances. However, like accuracy, may suitable unbalanced datasets. detailed diagnostics, including confidence intervals, specify detail = \"full\". formula PPV : $$PPV = \\frac{True Positives}{True Positives + False Positives}$$","code":""},{"path":[]},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/ppv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Positive Predictive Value (PPV, Precision) — ppv","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth,   threshold =     0.5, poslabel = 1 ) simple_ppv <- dx_ppv(cm, detail = \"simple\") detailed_ppv <- dx_ppv(cm) print(simple_ppv) #> [1] 0.8395062 print(detailed_ppv) #> # A tibble: 1 × 8 #>   measure           summary estimate conf_low conf_high fraction conf_type notes #>   <chr>             <chr>      <dbl>    <dbl>     <dbl> <chr>    <chr>     <chr> #> 1 Positive Predict… 84.0% …    0.840    0.741     0.912 68/81    Binomial… \"\""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/sensitivity.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Sensitivity (True Positive Rate, Recall) — sensitivity","title":"Calculate Sensitivity (True Positive Rate, Recall) — sensitivity","text":"Calculates Sensitivity, also known True Positive Rate (TPR) recall, proportion actual positives correctly identified classifier. Sensitivity key measure evaluating effectiveness classifier identifying positive instances.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/sensitivity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Sensitivity (True Positive Rate, Recall) — sensitivity","text":"","code":"dx_sensitivity(cm, detail = \"full\", ...)  dx_recall(cm, detail = \"full\", ...)  dx_tpr(cm, detail = \"full\", ...)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/sensitivity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Sensitivity (True Positive Rate, Recall) — sensitivity","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals. ... Additional arguments pass metric_binomial function, citype type confidence interval method.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/sensitivity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Sensitivity (True Positive Rate, Recall) — sensitivity","text":"Depending detail parameter, returns numeric value representing calculated metric data frame/tibble detailed diagnostics including confidence intervals possibly metrics relevant understanding metric.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/sensitivity.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Sensitivity (True Positive Rate, Recall) — sensitivity","text":"Sensitivity TPR important measure scenarios missing positive identification serious consequences. essentially measures proportion actual positives correctly identified, giving insight ability classifier detect positive instances. higher sensitivity indicates better performance recognizing positive instances. formula Sensitivity : $$Sensitivity = \\frac{True Positives}{True Positives + False Negatives}$$","code":""},{"path":[]},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/sensitivity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Sensitivity (True Positive Rate, Recall) — sensitivity","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth,   threshold =     0.5, poslabel = 1 ) simple_sensitivity <- dx_sensitivity(cm, detail = \"simple\") detailed_sensitivity <- dx_sensitivity(cm) print(simple_sensitivity) #> [1] 0.6938776 print(detailed_sensitivity) #> # A tibble: 1 × 8 #>   measure     summary       estimate conf_low conf_high fraction conf_type notes #>   <chr>       <chr>            <dbl>    <dbl>     <dbl> <chr>    <chr>     <chr> #> 1 Sensitivity 69.4% (59.3%…    0.694    0.593     0.783 68/98    Binomial… \"\""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/specificity.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Specificity (True Negative Rate) — specificity","title":"Calculate Specificity (True Negative Rate) — specificity","text":"Calculates Specificity, also known True Negative Rate (TNR), proportion actual negatives correctly identified classifier. Specificity key measure evaluating effectiveness classifier identifying negative instances.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/specificity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Specificity (True Negative Rate) — specificity","text":"","code":"dx_specificity(cm, detail = \"full\", ...)  dx_tnr(cm, detail = \"full\", ...)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/specificity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Specificity (True Negative Rate) — specificity","text":"cm dx_cm object created dx_cm(). detail Character specifying level detail output: \"simple\" raw estimate, \"full\" detailed estimate including 95% confidence intervals. ... Additional arguments pass metric_binomial function, citype type confidence interval method.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/specificity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Specificity (True Negative Rate) — specificity","text":"Depending detail parameter, returns numeric value representing calculated metric data frame/tibble detailed diagnostics including confidence intervals possibly metrics relevant understanding metric.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/specificity.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Specificity (True Negative Rate) — specificity","text":"Specificity TNR measures well classifier can identify negative instances, critical situations false positives carry high cost. higher specificity indicates better performance recognizing negative instances avoiding false alarms. formula Specificity : $$Specificity = \\frac{True Negatives}{True Negatives + False Positives}$$","code":""},{"path":[]},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/specificity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Specificity (True Negative Rate) — specificity","text":"","code":"cm <- dx_cm(dx_heart_failure$predicted, dx_heart_failure$truth,   threshold =     0.5, poslabel = 1 ) simple_specificity <- dx_specificity(cm, detail = \"simple\") detailed_specificity <- dx_specificity(cm) print(simple_specificity) #> [1] 0.9202454 print(detailed_specificity) #> # A tibble: 1 × 8 #>   measure     summary       estimate conf_low conf_high fraction conf_type notes #>   <chr>       <chr>            <dbl>    <dbl>     <dbl> <chr>    <chr>     <chr> #> 1 Specificity 92.0% (86.7%…    0.920    0.867     0.957 150/163  Binomial… \"\""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/summary.dx.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary — summary.dx","title":"Summary — summary.dx","text":"Summary","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/summary.dx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary — summary.dx","text":"","code":"# S3 method for class 'dx' summary(   object,   thresh = object$options$setthreshold,   variable = NA,   label = NA,   show_var = T,   show_label = T,   measure = NA,   ... )"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/summary.dx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary — summary.dx","text":"object object class \"dx\" thresh threshold return values variable Variable include returned values label Labels include returned values show_var Include variable column returned data? show_label Include label returned data? measure Measures include ... additional arguments passed methods","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/variance_a.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Variance of Cell Count — variance_a","title":"Calculate Variance of Cell Count — variance_a","text":"Calculates variance top left cell () count null hypothesis homogeneity odds ratios across strata.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/variance_a.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Variance of Cell Count — variance_a","text":"","code":"variance_a(tilde_a_value, marginals)"},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/variance_a.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Variance of Cell Count — variance_a","text":"tilde_a_value expected count cell '' single stratum. marginals list containing 'row' 'col' marginal sums stratum.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/reference/variance_a.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Variance of Cell Count — variance_a","text":"variance count cell ''.","code":""},{"path":"https://overdodactyl.github.io/diagnosticSummary/news/index.html","id":"diagnosticsummary-00200","dir":"Changelog","previous_headings":"","what":"diagnosticSummary 0.0.2.00","title":"diagnosticSummary 0.0.2.00","text":"New metrics Prevalence False Negative Rate False Positive Rate False Discovery Rate PR AUC Cohen’s Kappa Matthews Correlation Coefficient Balanced Accuracy F-beta F2 Informedness Markedness G-mean Fowlkes-Mallows Index Brier Score Detection Prevalence Information Rate (NIR) Also added aliases metrics name (e.g., recall true positive rate sensitivity) New/modified tests Breslow Day test implemented within package Added Chi-Square Test Added Fisher’s Exact Test Added G-Test (Log-Likelihood Ratio Test) Added McNemar’s Chi-squared Test Paired Proportions Added Z-test Comparing Two Proportions Added DeLong’s Test Comparing Two ROC Curves New output Added thresholds output dx containing metrics across full range thresholds Added prevalence output dx containing metrics across full range prevalencies Added rank output dx containing rank-based summaries predicted probabilities Changes dx_obj$measures output rawestime column renamed estimate rawuci rawlci column renamed conf_low conf_high New plots Added lift curve: dx_plot_lift Added Kolmogorov-Smirnov plot: dx_plot_ks Added cumulative gains chart: dx_plot_gain Added Precision-Recall plot: dx_plot_pr Added NPV PPV across prevalencies: dx_plot_predictive_value Added calibration curve: dx_plot_calibration Added decision curve: dx_plot_decision_curve Added Youden’s J Index Curve: dx_plot_youden_j Added Cumulative Accuracy Profile (CAP) curve: dx_plot_cap Added cost curve: dx_plot_cost Added plot showing metrics across thresholds: dx_plot_thresholds Add ability plot multiple ROC curves: dx_plot_rocs Renamed Functions confusion matrix plot renamed dx_cm dx_plot_cm (dx_cm now constructs confusion matrix) dx_roc renamed dx_plot_roc dx_forest renamed dx_plot_forest Bug Fixes Breslow-Day test longer silently left DescTools installed levels argument .data.frame summary didn’t filter label column Dependencies Removed dplyr, scales, boot, tidyr Imports Moved gtable, grid, gridExtra Suggests Removed DescTools e1071 Suggests Added tibble Suggests Comparison dx objects New dx_compare function run pairwise tests list dx_objects Documentation Added NEWS.md file track changes package. Added documentation/descriptions/details functions Migrated pkgdown website bootstrap 5","code":""}]
